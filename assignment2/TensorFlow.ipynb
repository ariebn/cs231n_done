{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arieb/Study/CS231n/assignment2/.env/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some useful utilities\n",
    "\n",
    ". Remember that our image data is initially N x H x W x C, where:\n",
    "* N is the number of datapoints\n",
    "* H is the height of each image in pixels\n",
    "* W is the height of each image in pixels\n",
    "* C is the number of channels (usually 3: R, G, B)\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, which needs spatial understanding of where the pixels are relative to each other. When we input image data into fully connected affine layers, however, we want each data example to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in TensorFlow -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up. \n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Hinge loss function, and the Adam optimizer being used. \n",
    "\n",
    "Make sure you understand why the parameters of the Linear layer are 5408 and 10.\n",
    "\n",
    ">>>> H = 32, HH=7 stride = 2 :   H_HAT = (H-HH + 1) / STRIDE = 13\n",
    ">>>> output vector size = 13 x 13 x 32 = 5408\n",
    "\n",
    "### TensorFlow Details\n",
    "In TensorFlow, much like in our previous notebooks, we'll first specifically initialize our variables, and then our network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on one epoch\n",
    "While we have defined a graph of operations above, in order to execute TensorFlow Graphs, by feeding them input data and computing the results, we first need to create a `tf.Session` object. A session encapsulates the control and state of the TensorFlow runtime. For more information, see the TensorFlow [Getting started](https://www.tensorflow.org/get_started/get_started) guide.\n",
    "\n",
    "Optionally we can also specify a device context such as `/cpu:0` or `/gpu:0`. For documentation on this behavior see [this TensorFlow guide](https://www.tensorflow.org/tutorials/using_gpu)\n",
    "\n",
    "You should see a validation loss of around 0.4 to 0.6 and an accuracy of 0.30 to 0.35 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False, plot_accuracy=False,mega_epoch=1):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    val_indices = np.arange(X_val.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    variables_no_train = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    Train_accuracy = []\n",
    "    Val_accuracy = []\n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        vcorrect = 0 \n",
    "        losses = []\n",
    "        vlosses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        Train_accuracy.append(total_correct)\n",
    "        \n",
    "        # check validation accuracy, do not train here !!!\n",
    "        for i in range(int(math.ceil(X_val.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_val.shape[0]\n",
    "            idx = val_indices[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: X_val[idx,:],\n",
    "                         y: y_val[idx],\n",
    "                         is_training: not(training_now)}\n",
    "            # get batch size\n",
    "            actual_batch_size = y_val[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            vloss, vcorr, _ = session.run(variables_no_train,feed_dict=feed_dict)\n",
    " \n",
    "            # aggregate performance stats\n",
    "            vlosses.append(vloss*actual_batch_size)\n",
    "            vcorrect += np.sum(vcorr)\n",
    "   \n",
    "        vtotal_correct = vcorrect/X_val.shape[0]\n",
    "        vtotal_loss = np.sum(vlosses)/X_val.shape[0]\n",
    "        Val_accuracy.append(vtotal_correct) \n",
    " \n",
    "        if training_now:\n",
    "                print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "                  .format(total_loss,total_correct,int((mega_epoch * epochs)+e+1)))\n",
    "        else: \n",
    "            \n",
    "                print(\"Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "                 .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss, total_correct, Train_accuracy, Val_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a specific model\n",
    "\n",
    "In this section, we're going to specify a model for you to construct. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the TensorFlow documentation and configuring your own model. \n",
    "\n",
    "Using the code provided above as guidance, and using the following TensorFlow documentation, specify a model with the following architecture:\n",
    "\n",
    "* 7x7 Convolutional Layer with 32 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Affine layer with 1024 output units\n",
    "* ReLU Activation Layer\n",
    "* Affine layer from 1024 input units to 10 outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 11.5 and accuracy of 0.078\n",
      "Iteration 100: with minibatch training loss = 0.927 and accuracy of 0.31\n",
      "Iteration 200: with minibatch training loss = 0.723 and accuracy of 0.42\n",
      "Iteration 300: with minibatch training loss = 0.783 and accuracy of 0.31\n",
      "Iteration 400: with minibatch training loss = 0.564 and accuracy of 0.34\n",
      "Iteration 500: with minibatch training loss = 0.537 and accuracy of 0.33\n",
      "Iteration 600: with minibatch training loss = 0.556 and accuracy of 0.28\n",
      "Iteration 700: with minibatch training loss = 0.385 and accuracy of 0.41\n",
      "Epoch 2, Overall loss = 0.769 and accuracy of 0.312\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8lNW9+PHPNwtJIEDYDDuIIIgLSxBRUXEHFLXW3etW\nKretrbb2VlFrvf1ZldpbrbbW1q1FrxW9rhSpFjGIGyo7yL5vgbBk35P5/v54zgxDyDIzYTIT8n2/\nXvOa5znPMt/JMt855zzPOaKqGGOMMaFIiHUAxhhjWg5LGsYYY0JmScMYY0zILGkYY4wJmSUNY4wx\nIbOkYYwxJmSWNIwJk4ioiAyMdRzGxIIlDdOiicgWESkTkeKgx59iHZefiJwkIh+KyD4RafSmKEtI\nJt5Z0jBHg0mqmh70+HGsAwpSBbwBTI51IMYcCZY0zFFLRG4Vkc9F5E8iUiAia0Tk/KDtPUVkpogc\nEJENInJ70LZEEblfRDaKSJGILBKRPkGnv0BE1otIvog8IyJSVwyqulZVXwS+beJ7SRCRX4rIVhHJ\nFZGXRaSj25YqIv8rIvtdPN+ISGbQz2CTew+bReTGpsRhjCUNc7Q7DdgIdAUeAt4Wkc5u2wxgB9AT\nuAp4VETOc9vuBq4HJgIdgO8BpUHnvRQ4FTgFuAa4OLpvg1vd41xgAJAO+JvhbgE6An2ALsAPgDIR\naQc8DUxQ1fbAGcDSKMdpjnKWNMzR4F33Ddv/uD1oWy7wB1WtUtXXgbXAJa7WcCZwr6qWq+pS4AXg\nZnfc94FfupqCquoyVd0fdN5pqpqvqtuAbGB4lN/jjcATqrpJVYuB+4DrRCQJrwmsCzBQVWtUdZGq\nFrrjfMBJIpKmqjmq2qQajzGWNMzR4ApVzQh6PB+0baceOirnVryaRU/ggKoW1drWyy33wauh1Gd3\n0HIp3jf/aOqJF5/fViAJyAReAT4EZojILhF5XESSVbUEuBav5pEjIu+LyJAox2mOcpY0zNGuV63+\nhr7ALvfoLCLta23b6Za3A8c1T4gh2QX0C1rvC1QDe1wt6teqOhSvCepSXI1JVT9U1QuBHsAa4HmM\naQJLGuZodwxwp4gki8jVwAnAbFXdDnwBPOY6kk/Bu8Lpf91xLwAPi8gg8ZwiIl3CfXF3bCrQxq2n\nikhKI4e1cfv5H4nAa8DPRORYEUkHHgVeV9VqETlXRE52+xXiNVf5RCRTRC53fRsVQDFec5UxEUuK\ndQDGHAH/FJGaoPU5qvodt/wVMAjYB+wBrgrqm7ge+Avet/g84CFV/chtewJIAf6N14m+BvCfMxz9\ngM1B62V4TUv9Gzimdr/D7cBLeE1U84FUvOaon7jt3d376I2XGF7Ha7Lqhteh/zKgeJ3gP4zgPRgT\nIDYJkzlaicitwPdVdWysYzHmaGHNU8YYY0JmScMYY0zIrHnKGGNMyKymYYwxJmQt+uqprl27av/+\n/SM6tqSkhHbt2h3ZgI4gi69p4jm+eI4NLL6magnxrVmzZp+qdovoBKraYh9ZWVkaqezs7IiPbQ4W\nX9PEc3zxHJuqxddULSE+YKFG+LlrzVPGGGNCZknDGGNMyCxpGGOMCZklDWOMMSGzpGGMMSZkljSM\nMcaEzJKGMcaYkLXKpPHNlgO8vb6SymqbWsAYY8LRKpPGoq15zNxYRbXPkoYxxoSjVSYN/9yfNlaj\nMcaEp1UmDWOMMZFplUlDXFXDKhrGGBOe1pk0XAOVWvuUMcaEpXUmDatpGGNMRFpl0jDGGBOZqCUN\nERksIkuDHoUi8lMR6Swic0RkvXvu5PYXEXlaRDaIyHIRGRmt2PysdcoYY8ITtaShqmtVdbiqDgey\ngFLgHWAqMFdVBwFz3TrABGCQe0wBno1WbGLtU8YYE5Hmap46H9ioqluBy4Hprnw6cIVbvhx42U0u\ntQDIEJEe0QgmcJ+GZQ1jjAmLNMcVRCLyErBYVf8kIvmqmuHKBchT1QwRmQVMU9XP3La5wL2qurDW\nuabg1UTIzMzMmjFjRtjxzNlSxatrKvnTeW1JbyONHxADxcXFpKenxzqMell8kYvn2MDia6qWEN+k\nSZMWqeqoiE4Q6TyxoT6ANsA+INOt59fanueeZwFjg8rnAqMaOnekc4T/7bNN2u/eWXqguCKi45tD\nS5hnOJ7Fc3zxHJuqxddULSE+4nyO8Al4tYw9bn2Pv9nJPee68p1An6DjeruyqLHGKWOMCU9zJI3r\ngdeC1mcCt7jlW4D3gspvdldRjQEKVDUnGgH5O8LVLp8yxpiwJEXz5CLSDrgQ+M+g4mnAGyIyGdgK\nXOPKZwMTgQ14V1rdFr24vGdLGcYYE56oJg1VLQG61Crbj3c1Ve19FbgjmvH42Si3xhgTmdZ5R7jE\n5xVTxhgT71pn0nDsPg1jjAlPq0wagXqG5QxjjAlL60wa1hFujDERaZ1JIzCfRowDMcaYFqZ1Jg3r\nBzfGmIi0yqThZx3hxhgTnlaZNOw+DWOMiUzrTBrWEW6MMRFpnUkDG3vKGGMi0SqTBtYRbowxEWmd\nScOxioYxxoSnVSYNq2gYY0xkWmfSELu5zxhjItE6k4Z7tvs0jDEmPK0zaVj7lDHGRKRVJg0/a54y\nxpjwtMqkYTf3GWNMZFpn0rCb+4wxJiJRTRoikiEib4rIGhFZLSKni0hnEZkjIuvdcye3r4jI0yKy\nQUSWi8jI6MXlPVvKMMaY8ES7pvEU8IGqDgGGAauBqcBcVR0EzHXrABOAQe4xBXg2yrEZY4wJU9SS\nhoh0BM4GXgRQ1UpVzQcuB6a73aYDV7jly4GX1bMAyBCRHtGKz4spmmc3xpijj0SrXV9EhgPPAavw\nahmLgLuAnaqa4fYRIE9VM0RkFjBNVT9z2+YC96rqwlrnnYJXEyEzMzNrxowZYce2IKeavyyr4NGx\nafRMj89uneLiYtLT02MdRr0svsjFc2xg8TVVS4hv0qRJi1R1VEQnUNWoPIBRQDVwmlt/CngYyK+1\nX557ngWMDSqfC4xq6DWysrI0EjOX7tR+987SdbsLIzq+OWRnZ8c6hAZZfJGL59hULb6magnxAQs1\nws/2aH7N3gHsUNWv3PqbwEhgj7/ZyT3nuu07gT5Bx/d2ZUecdYQbY0xkopY0VHU3sF1EBrui8/Ga\nqmYCt7iyW4D33PJM4GZ3FdUYoEBVc6IRm9iQhcYYE5GkKJ//J8CrItIG2ATchpeo3hCRycBW4Bq3\n72xgIrABKHX7RpV1hBtjTHiimjRUdSle30Zt59exrwJ3RDMev4PNU5Y1jDEmHPF56VCUBUa5tZxh\njDFhaZ1Jw1/TsKRhjDFhaZVJw+buM8aYyLTSpOGxPg1jjAlPq0wa1jxljDGRaZ1JI9YBGGNMC9U6\nk4b459OIcSDGGNPCtM6kEesAjDGmhWqVScPPOsKNMSY8rTJpWEe4McZEpnUnjdiGYYwxLU7rTBr4\nO8ItbRhjTDhaZdKwnnBjjIlM60wajtUzjDEmPK0yadgot8YYE5nWmTQkkDZiGocxxrQ0rTNpuGer\naRhjTHhaZ9KwjnBjjIlIVJOGiGwRkRUislREFrqyziIyR0TWu+dOrlxE5GkR2SAiy0VkZDRjA2uc\nMsaYcDVHTeNcVR2uqv65wqcCc1V1EDDXrQNMAAa5xxTg2WgFdPA+jWi9gjHGHJ1i0Tx1OTDdLU8H\nrggqf1k9C4AMEekRjQAODiNiWcMYY8IhjX1wishdwN+AIuAFYAQwVVX/3ejJRTYDeXgtQX9V1edE\nJF9VM9x2AfJUNUNEZgHTVPUzt20ucK+qLqx1zil4NREyMzOzZsyYEdYbBli1v4bHvyln6uhUhnRO\nDPv45lBcXEx6enqsw6iXxRe5eI4NLL6magnxTZo0aVFQ6094VLXBB7DMPV8MvA2cCCxu7Dh3TC/3\nfAywDDgbyK+1T557ngWMDSqfC4xq6PxZWVkaic837NV+987SLzfui+j45pCdnR3rEBpk8UUunmNT\ntfiaqiXEByzUED7D63qE0jzlv9ZoIvCKqn5LiANxqOpO95wLvAOMBvb4m53cc67bfSfQJ+jw3q4s\naqx1yhhjwhNK0lgkIv/GSxofikh7wNfYQSLSzu2LiLQDLgJWAjOBW9xutwDvueWZwM3uKqoxQIGq\n5oT1bkIU6Ai366eMMSYsSSHsMxkYDmxS1VIR6QzcFsJxmcA77u7rJOAfqvqBiHwDvCEik4GtwDVu\n/9l4iWkDUBria0TEbgg3xpjIhJI0TgeWqmqJiPwHMBJ4qrGDVHUTMKyO8v3A+XWUK3BHCPE0meUM\nY4yJTCjNU88CpSIyDPg5sBF4OapRRZnYLeHGGBORUJJGtasFXA78SVWfAdpHN6zmYR3hxhgTnlCa\np4pE5D7gJuAsEUkAkqMbVnQdnO7VsoYxxoQjlJrGtUAF8D1V3Y13KezvohpVlNkot8YYE5lGk4ZL\nFK8CHUXkUqBcVVt4n4b3bDnDGGPC02jSEJFrgK+Bq/Euj/1KRK6KdmDRZR3hxhgTiVD6NB4ATnV3\ndSMi3YCPgDejGVhzUGufMsaYsITSp5HgTxjO/hCPi1vWPGWMMZEJpabxgYh8CLzm1q/Fu3u7xQo0\nTlnWMMaYsDSaNFT1FyLyXeBMV/Scqr4T3bCiy39zn11ya4wx4QmlpoGqvgW8FeVYmo11gxtjTGTq\nTRoiUkTdDTiCN1RUh6hF1UysH9wYY8JTb9JQ1aNiqJC6HJzuNbZxGGNMS9Oir4KK1MH5NIwxxoSj\ndSaNQE3D0oYxxoSjVSYNY4wxkWnVScPqGcYYE55Qxp66UkTWi0iBiBSKSJGIFDZHcNFiHeHGGBOZ\nUO7TeByYpKqrox1McxGb8NUYYyISSvPUnqYkDBFJFJElIjLLrR8rIl+JyAYReV1E2rjyFLe+wW3v\nH+lrNh6T92w1DWOMCU+9ScM1S10JLHQf5tf7y1x5qO4CgpPOb4EnVXUgkAdMduWTgTxX/qTbLyps\ninBjjIlMQzWNSe7RASgFLgoquzSUk4tIb+AS4AW3LsB5HBxWfTpwhVu+3K3jtp8vEt2Pd6toGGNM\neCSa9yqIyJvAY0B74L+AW4EFrjaBiPQB/qWqJ4nISmC8qu5w2zYCp6nqvlrnnAJMAcjMzMyaMWNG\n2HHtKPLxy8/L+NHwFEZ3D2n4rWZXXFxMenp6rMOol8UXuXiODSy+pmoJ8U2aNGmRqo6K6ASq2uAD\n79t/RtB6J+ClEI67FPizWx4HzAK6AhuC9ukDrHTLK4HeQds2Al0beo2srCyNxNrdhdrv3ln6z2U7\nIzq+OWRnZ8c6hAZZfJGL59hULb6magnxAQu1kc/w+h6hfM0+RVXzg5JMnoiMCOG4M4HLRGQikIrX\nzPUUkCEiSapaDfQGdrr9d7okskNEkoCOeBM+HXGBa6esfcoYY8IS0sx9ItLJvyIinQltHo77VLW3\nqvYHrgM+VtUbgWzAP8f4LcB7bnmmW8dt/1g1Oh/r1hFujDGRCaWm8XvgSxH5P7d+NfBoE17zXmCG\niPwGWAK86MpfBF4RkQ3AAbxEE1VW0TDGmPCEUmN4WUQW4l31BHClqq4K50VUdR4wzy1vAkbXsU85\nXkJqBuJ/zeZ5OWOMOUo0mjRE5BVVvQlYVUdZi2TNU8YYE5lQ+jRODF4RkUQgKzrhNA/rCDfGmMg0\ndEf4fW7K11OCBiosAnI52HndIkX5nkFjjDlq1Zs0VPUx9aZ8/Z2qdlDV9u7RRVXva8YYo0atK9wY\nY8IS0qWz7pLbQXj3W/jL50czsGiy5iljjIlMKB3h38cbdLA3sBQYA3zJwaupWhwb5dYYYyITSkf4\nXcCpwFZVPRcYAeQ3fEh888+nYTnDGGPCE0rSKHf3UCAiKaq6Bhgc3bCiy/rBjTEmMqHcEb5DRDKA\nd4E5IpIHbI1uWM3Dbu4zxpjwhNIR/h23+N8iko03kOAHUY2qmVjKMMaY8IQ0mYSIjATG4n3Ofq6q\nlVGNKsrEpgg3xpiINNqnISK/wptTowvefBh/E5FfRjuwaPLf3Gf3aRhjTHhCqWncCAwL6gyfhnfp\n7W+iGVg0WT+4McZEJpSrp3YRdFMfkMLBiZNaNOsHN8aY8NRb0xCRP+K1+hcA34rIHLd+IfB184QX\nHYGb+2IbhjHGtDgNNU8tdM+LgHeCyudFLZpmEri5z7KGMcaEpd6koarTmzOQ5nSwpmFZwxhjwtFQ\n89QbqnqNiKygjpYcVT0lqpEZY4yJOw01T93lni+N5MQikgrMx+s4TwLeVNWHRORYYAbeJbyLgJtU\ntVJEUoCX8SZ42g9cq6pbInntRmNzz9Y8ZYwx4WloPo0c97y1rkcI564AzlPVYcBwYLyIjAF+Czyp\nqgOBPGCy238ykOfKn3T7RYfLGou25rFlX0nUXsYYY442odzcd6WIrBeRgqAZ/AobO049xW412T0U\nb0j1N135dOAKt3y5W8dtP1+iNMWevyP8nSU7Gfc/86LxEsYYc1QK5T6Nx4HLVLVj0Ax+HUI5uYgk\nishSvCli5wAbgXxVrXa77AB6ueVewHYAt70ArwnriKudimp81k5ljDGhCOWO8D2qujqSk6tqDTDc\njZL7DjAkkvMEE5EpwBSAzMxM5s2bF/Y5CisOTRL//ngeaUnxdZ94cXFxRO+tuVh8kYvn2MDia6qW\nEF9ThJI0ForI63hDo1f4C1X17VBfRFXz3Qi5pwMZIpLkahO9OXh3+U6gD95Q7El4o+nur+NczwHP\nAYwaNUrHjRsXahgB+4srIPujwPrLm9J4bcqYsM8TTfPmzSOS99ZcLL7IxXNsYPE1VUuIrylCaZ7q\nAJQCFwGT3KPRK6pEpJurYSAiaXh3kq8GsoGr3G63AO+55ZluHbf9Y43ShBe1u0q+3HRYbjLGGFOH\nUObTuC3Cc/cApotIIl5yekNVZ4nIKmCGiPwGWAK86PZ/EXhFRDYAB4DrInzdRsVXQ5QxxrQcDd3c\nd4+qPh40BtUhVPXOhk6sqsvx5hOvXb4JGF1HeTlwdShBN5VN92qMMZFpqKbh7/xe2MA+Rw1VPazZ\nyhhjzKEaGnvqn+75qBuDSupooKqo9pGanBiDaIwxpuVotE9DREYBDwD9gvdvyWNPSR3d/2WVNZY0\njDGmEaFccvsq8AtgBeCLbjjNo03i4VmjtKqGTjGIxRhjWpJQksZeVZ0Z9UiaUV1Jo6yyuo49jTHG\nBAslaTwkIi8Ac4nw5r54k5BweJ9GaWVNDCIxxpiWJZSb+27DjVJLGDf3tRT3TfBGNimzpGGMMY0K\npaZxqqoOjnokMdIhLRmAypqjorvGGGOiKpSaxhciMjTqkcRIh1SXNKotaRhjTGNCqWmMAZaKyGa8\nPg3Bmy6jxV5yG6xDmvcjsKRhjDGNCyVpjI96FDHkr2lUWNIwxphGhTJgYShTu7ZYgT4NSxrGGNOo\nUPo0jmrtUry7wCusI9wYYxrV6pNGSpJLGlV2ya0xxjTGkkaS9yOwS26NMaZxrT5p+IcUsT4NY4xp\nXKtPGv4hReav2xvjSIwxJv6FcsntUenmoW1I6tQrsL54Wz4V1TWBPg5jjDGHa7VJ47y+yYwbd+iN\n7sXl1aSkW9Iwxpj6RK15SkT6iEi2iKwSkW9F5C5X3llE5ojIevfcyZWLiDwtIhtEZLmIjIxWbPUp\nqbArqIwxpiHR7NOoBn6uqkPxhiK5w41hNRWYq6qD8IZbn+r2nwAMco8pwLNRjO0Qo/t3BqC4wubU\nMMaYhkQtaahqjqoudstFwGqgF3A54J93fDpwhVu+HHhZPQuADBHpEa34gt15/iDAkoYxxjRGVDX6\nLyLSH5gPnARsU9UMVy5AnqpmiMgsYJqqfua2zQXuVdWFtc41Ba8mQmZmZtaMGTMiiqm4uJj09HQA\nNubX8PCCcn6WlcKwbvHRzRMcXzyy+CIXz7GBxddULSG+SZMmLVLVURGdQFWj+gDSgUXAlW49v9b2\nPPc8CxgbVD4XGNXQubOysjRS2dnZgeV1uwu1372zdObSnRGf70gLji8eWXyRi+fYVC2+pmoJ8QEL\nNcLP9KjepyEiycBbwKt6cHrYPf5mJ/ec68p3An2CDu/tyqIuPdWrXZRY85QxxjQomldPCfAisFpV\nnwjaNBO4xS3fArwXVH6zu4pqDFCgqjnRii9Y22Qvadg84cYY07BoNuCfCdwErBCRpa7sfmAa8IaI\nTAa2Ate4bbOBicAGoBRvbvJmkZLs5c7yaksaxhjTkKglDfU6tKWezefXsb8Cd0Qrnob4By2sqLLx\np4wxpiGtfuwpABGhTVKC1TSMMaYRljSc1KQEq2kYY0wjLGk4KcmJVFhNwxhjGmRJw0lNtpqGMcY0\nxpKGk5KUSHl1DbmF5Ww/UBrrcIwxJi7Fx5gZcSA1OYHZK3Yze8VuurVP4ZsHLoh1SMYYE3espuEk\nysGrg/cWVcQwEmOMiV+WNJxtQU1SKUkJ/Oz1pazdXRTDiIwxJv5Y0nDySqsAuDqrNxXVPt5ZspPH\nP1gT46iMMSa+WJ+G88wNI9mVX0ZQK5WNRWWMMbVY0nAuOcWb7+mNb7YHyvYVW9+GMcYEs+apWjqk\nHcyjJRXV5BaWszO/LIYRGWNM/LCkUcuYAV0Y0LUd4E3/es1fv+TMaR9TXmVNVcYYY0mjloy2bfj4\nv8Zxx7nHUVJZw5b93lVVi7flxTgyY4yJPUsa9WiXkkSNT+ncrg0Aq3YVxjgiY4yJPUsa9Wif4vVt\n1PgUgE37SmIZjjHGxAVLGvVIa+MljYIy7/6N4nKbP9wYYyxp1KNLeptD1osrLGkYY4wljXqcM6gb\nfTqnBdZ35JVyoKQyhhEZY0zsRS1piMhLIpIrIiuDyjqLyBwRWe+eO7lyEZGnRWSDiCwXkZHRiitU\nCQnCxUO7B9bX7SnmoifnxzAiY4yJvWjWNP4OjK9VNhWYq6qDgLluHWACMMg9pgDPRjGukPXISDtk\nfV9xBVv3H+wQL62strk3jDGtStSShqrOBw7UKr4cmO6WpwNXBJW/rJ4FQIaI9IhWbKHq0TH1sLKc\ngnJ2F5RTVlnDFc98zlmPZ8cgMmOMiQ1R1eidXKQ/MEtVT3Lr+aqa4ZYFyFPVDBGZBUxT1c/ctrnA\nvaq6sI5zTsGrjZCZmZk1Y8aMiGIrLi4mPT29wX3yK3z8NLvuIUSO75TAujxvetiXLm5Lggh7Snzc\n+2kZd2elcEq3pg3rFUp8sWTxRS6eYwOLr6laQnyTJk1apKqjIjk+ZgMWqqqKSNgZS1WfA54DGDVq\nlI4bNy6i1583bx6hHHvFxV4z1JrdRVz55y8C5f6EATDq9LF0SE3mrUU7gGVsrOnCneNGHHIen0/Z\nnldKvy7tjmh8sWLxRS6eYwOLr6laQnxN0dxXT+3xNzu551xXvhPoE7Rfb1cWF9q2SWJY74x6t+cW\nVnDHq4t5/tNN9e5z/zsrOOd388gv9a7A2pBbzCPvr8Lni15NzxhjjrTmThozgVvc8i3Ae0HlN7ur\nqMYABaqa08yxNSgxQerddsETn/D+ihzWuJn+Kqt9PP7BGvJKKvl0/V5qfMoMN+S6fyrZKS8v5PlP\nN9sIusaYFiVqzVMi8howDugqIjuAh4BpwBsiMhnYClzjdp8NTAQ2AKXAbdGKqyneu+NMLn/m88D6\nD845jr98svGw/eas2kO1T3lz0Q5yiyr48bkDA9v893r4R82tqPYddrwxxsSrqCUNVb2+nk3n17Gv\nAndEK5YjZVifDC4cmsmcVXsAGNy97s6uatfklOtqFX/K3hDY9uB7KxmU2Z5dBeWAN0xJRXUNKUmJ\ndZ7rjMfmctGJ3fnvy048Yu/DGGMiZXeEh+mH444LLPs7tX9wznG89cMzQjp+3Z5i3l9+sOXtx/9Y\nzOBffsDdry89ZKiS8mpldU4huwrK+fsXW1i3p8hmEjTGxJwljTCN7NuJCSd1Dyy/86Mz+MXFg8nq\n14n7Jw7hrEFdAbhwaGZI58txNY63l+xkwcb9AOSXVvKDj0qZ8NSngf0uenI+Nzy/gBc+3cTxD/yL\naF4qbYwx9bE5wiPw1HUjmFbp9UmM6NspUD7l7OPomZHGp+v3MW5wt0Azlt/Ivhks3pZf73m///JC\nrs7qza6CujvH1+0p5jfvrwZgT2EFG3KLqarxce6QY0KKu8anqCpJifZdwRgTGUsaEWiTlECbpLo/\neC85uQcjp3aiZ0YaqUmJLNmex53nDWL++n1cPrwnH6/JZdbyHArLqvhk3V6Oz0wnMSGB1TneJE//\nt2hHSDGMeWxuYHnLtEtYnVNIz4w0anxKYoLQPiWJhAShstrHq19t5cqRvbn5pa9Blfd+PJaSimoe\nnb2an114PF3TU8L+Gewq9rFiRwEn9+4Y9rHGmJbLksYRJiL0dGNWfTerN9/N6g3AVe754hO7c/GJ\nXvNWWWUNqckJiAj9p74f8Wv6fMqEpz4lJSkhcDVW705pDOudwfsrvP6Tj1bvYdl2r5Yz9a3lgUuA\n312yk8lnDeDuC48/5JylldXc9OLXtE9N4vmbR1FV4+PJOetYs7uI318zjPs/K4PPPmPLtEsijtsY\n0/JY0oihtDaHXjGVmCBsfHQiz83fyKOz1zR47FVZvXnT1UomPu31fQRfvrsjr4wdeQebuT7fsD+w\n7E8YACWVNTw9dz0Dj0lnwkndeXfJTtbsLuLLjftZ5Wo/H6zcTfaaXN5e4t1vefOLXweO31tUgaoy\nb+1eLj6xOx3bJnOgpJL2qUkI8PTHG/iP0/pyTAdvHK/Kah9Lt+cz+tjOIf+cjDHxw5JGnJj783No\nn+r9OqacfVwgaYzsm8HksQO44x+LAXjy2mGc0KMDB0oqA0nDf1Oh3ws3j2LOqj28vnA7tfXr0pat\n+w8fmffO15Yc1ufSPiWJoopqVu4qIHttbqA8+PVOfeSjg+9hzR7+eP1Ixjw6l/5d27KnsIKCsiq+\n2XyAf9x+GjkF5Yz/w3wKy6t5ZfJoTumVQce2yQDkFpXzz2U53HZGfxLcjZTbD5TSo2NqvX0w/osB\nvGHMDlqzu5AffVTCByeX0rdL2zqPrU+NT3n1q61cMaIXHVKTwzrWmNbAkkacOK7bofd8/PqMVC46\n5wx6dPRrNEZoAAAV+UlEQVSauk4bcAFrcooY667O8vmUcYO7MW/tXgDOPr4b89d5y/26tGXad0/m\nwqGZfP/lhaQlJ1LmbiZ8+Xuj8Sm0S0kkPSWJX7y5PHAJcO1O+r/ddip3vraEv37iDY9y+1nH8vyn\nmwPbO6QmURg0De6H3+7h9MfmUlnjY92e4kD5l5v2c+1fF/D1loODHt/kaivZ/zWOY7u24543lzNv\n7V4enrWKvp3b8tKto7jgifm0a5PIzJ+MPeznA3DjC19R7VNG9M2gZ8c0bjmjP3kllYz/g1fzeuDd\nFbwy+bTDjludU8iv3lvJry49kcyOKRzT/uBoxgs27edX733L/HV7eeGWUwPl+aWVFJZVh52EIlFe\nVUNqct337Vz558/pkZHGMzccmSlnyqtqyCutDPydGdMYSxpxql+HxEP+kbumpzB20MEO64QE4e+3\njWbR1jzW7yniutF9+f2/17KvuIKBx6QjIlwwNJPP7j2Xbu1TGPzLDwDo3jH1kBsJH7vy5EPuGwkW\nfBMiwP0TTwgkjYwUIfsX5/K3zzfz9McHb17cX2t2w6QEodqnhySMYL98dwVXZ/UJJD+AbQdKueAJ\nb8Krksoazv/9J4w/sTtd27dhQNd03lmykz/fOJIv3CXKX2/2zr10ez5rg2pBn67fR2llNUkJCXy+\ncR9F5dWoKku25fPNljwm/ekzAKacPYDendL4Zksep/TyOvbX7C5i7e4iqmp8DDwmnUv/+Bk78srY\n/NhEvtp8gNH9O1NeXcPKnYWHNbXNXLaLX723kgX3nR/48C+pqGb6l1uYctaAOn8Ofm8v3sHdbyzj\n03vOpU/nwxPU4m35sC2fZ27w1pdtz+fZeRt5+voR9V6c0ZCfv7GM91fksOGRCXZVnQmJJY0WLqtf\nJ7L6eZf9/vyiwYdt792pbWC/RVvzDrvzvENqMpsencj2vFKmvrWCLzcd7PvomJbMLy4ezO8+XMvv\nrjoFEeE3V5zE/y7Yys0Dq+nUrg2DMtsD8PhVp3DPm8sB+O7I3ry12DWdPTyegQ/8q974P9+wP9Df\nkpwoVNV4TU5d01MOuZnxg293H3LcjG+2HXaud5YcPsbl0F99yK8vO5GHZn5bbwzPzT840ORsd+HA\njrwyLv6Dl7gyO6Swp9CL5d2lO/nZ68s4PjOd6hpl074S7p84hDW7i7jn4iGkJCVw52tLABjy4AfM\n+dnZ9MhI46SHPvR+Th+s5e6sFMbVEcemvcXc/cYyANbnFtGnc1t8PmXz/hIy0pJJqaP2cdeMJWzZ\nX8pDM79l/EndOef4bvW+T4DqGh+JCRJo0vNfKLGvuJLubv6Y4kplf3EFXRq5qk5V2VNYETgOoKrG\nR1LQ+RuzIbeYovIqenRM45j2KYGmSXO48qoaduaX1Vnrbk5RnU8j2kaNGqULFx425UZIWsLwxUcy\nvvKqGoorqhu8vNbnU34yYwnvL88hOVFY/8jERuPz+ZRVOYWc1Ktj4Aqwt354BjvySqmqUa7K6s29\nby7n9YXbaZ+axEu3nsr7y3P4+xdbDjlf705pfHrPuRx732wA1v1mApv2FdO/Szs+XpPLj15dHNL7\nPOO4LnyxcT/d2wq7S+v/2z7t2M585WooN5zWlze+2R4Y/qU5/PH6ESjw3PyNPP7dYQzt2YGTHvow\nMCrA6P6d2VdSwcm9OvLe0l2HHX/DaX0pKKti9oocgv+F7xk/mJ15ZZRV1rB6dxGrcwq58bS+pKck\nMXXCEAY98C++M6IXN5zWl9krcgI1x2lXnsxvP1jDmAFd+NfK3bRPSeKSU3pw+9kDAh9Se4sq6JCW\nRGFZNd3ap/DEnHU8PXc9147qQ4+MVC4a2p2JT39K9w6pPPKdkzj/hEyem7+RLzfu59whx7BwSx5P\nXjuc+ev2ctvfv+HSU3rw8ZpcSt09Tz8cdxz3jh8SeC+vf7ONId07MKyPN8J0YXkVOfnlfPz513zn\ngjNJShTKKmsCNTJVpaiimmv/uoDvndmfq0f1obiimqQEYcm2fDqkJXFiz/ovEd9bVEFJRTX9u4Y2\nfUF9IvnfrfEp76/IYeJJ3Q+p8c1ekcPpA7qweFsek6d7n3Wr/9/4wy6iCTe+c889N+L5NCxpxKlY\nxjd/3V66pqcwtGeHevepKz5/0ljy4IV0atcmUO7zKQVlVSQnJZCekkRVjY8/Z2/kutF9ELyryHw+\n6Ng2me0HSklJSghcbQXet2N/baVP5zSO7Zoe6L+ZPPZYXvzM++D7z7MHMHXCEIorqvni8894cEFN\nYPyvhy8/kax+nbn95YUM75PBw1ecxPenf8OjV57MkO4d8PmUAfd7CetXlw7l2U82UlpRTYn7QIu2\nAd3asWlvSeM7NsHsO88KXGkXjp+cN5BVuwqZuyaXzu3aBAbdbMyAru3YtO/Q9/T8zaO4/eW6/2eT\nEoQNj07kQEklm/YWc9Vfvgxs+8O1w/np60vrPO4H5xzH1AlDePDdlbyyYGugfPr3RnPPm8uoqPaR\nX1oFcNgl4kXlVSzbXkC/Lm0Ds3B+/cD57C2qYOPeEnw+5YoRvQL3P9U2d/UeBh6TTq+MNJ6Ys46M\ntskMqN7G2WefQ1lVDaWV1Tw9dz0JIjzynZP5dlcBa3KKyGibzCm9MyiuqGbzvmI+37CfFz/bzB+u\nHc4VI3oBkFNQxumPfXzYaz5zw0iG9uzAsREmN0saljRioq745q3NZebSXTxx7fAj/nr+hLRl2iXU\n+JTTHv2Ivp3b8sZ/ns49by1nSPf23H7WgECziD++iuoacgsr6uwfqG1DbhFPfrSe3189LNAXUVhe\nxf/75yreXLSDId3b8x9j+pHVrxP7iit48N2VvHjrqXRrn8LugnLW7i7iy037+cdX27hyRC/uPH8Q\nN77wFTvzy7jk5B68vyKHzA4pZHXxMXtzVYOxdEhNYkC3dJa6e2semHgCC7ce4MNv9xy278Bj0tmQ\ne/DCAxFviJtFW/MO2e+c47vxybq9tQ+PK/Vd3deYUJNu2zaJJCUIx3ZLp12bxEC/WEN+ct5Anv90\nEyP7duLcwcdw0+n9mPavNfTp3JaHZ60CIEGgsYpqKD//M47rQmKCsHhrHn06tz3syshg6x+ZQHIE\n/VCWNCxpxERzx7dyZwEd05IDH/5llTUkJUq9/zRHOr63Fu1gzHFd6JUR+VVG5VU1JCUIn306n4HD\nRpNfWkXPjDTeX5HDg++upF2bRB698mQuPrE7qpCSlMC8dbmMHdgt0Mm9M7+MGV9vY9zgY7j95YUc\nKKnk77edSmW1jz/P28jS7fmBTu0NuUWBCwr82qcm0a+Ldzn0ZcN6MmfVHiaPPZa+ndsydlBXXvh0\nM7p/C+PPPo0Fmw5w/zsrDnsfD0w8gde+3saDk4Zy29++CZT/59kD+Ov8TZzQowO3ntGPPp3b8tjs\nNWzILaasqibQr1Zbj46pgTHYYkGEQBNf2zaJgeayeHF8ZjoJIoclkPoulmhMU5OGdYSbFuGkXoe2\nRTelTTcS/jv7myL4MtrendrS2w1bdtOYflw2rCcogftW/M4bcujAl70y0gIXPEy/bTRLtudx9qBu\nJCQIZw7sSllVTaBNfOAx3kUKI/pm0KVdCh+t3sO/7jorcHEEwIOXDj3k/D8cdxzz5m1nQLd0BnRL\n55JTevCz15fyo3HHcWLPjuwuLOfYru24/ewB1AR9tf7j9SOYNKwnUycMOaQT/J8/GUteSSVfbd7P\n+JN6sLugnOKKagZ0bcePXl3MvHW5vHTrqZzQowNPfbSe5Tu8Gz87pCXTPjWJS07uwZb9pVzwxCdc\nOaIXv7t6GNnZ2WxM6kdKciJb9pUEmif9nr5+BN07pFJSWU1GWjLfbDnA8h0F/GrSULq0S6Ha52Pd\n7mL+9sVmrhnVh+F9Mhjy4AeM6teJO88fxM0vfc3FJ2ZSXFHNmGO7sGJnAXecO5Al2/JYsj2fTXtL\nWLGzIPB6L906it//ex1llTWB5rgrR/QK3AwLXpNStc9H/y7t+HLTfv53wVauzurD6cd14Zq/fnlI\n/BefmMn5J2Tyuw/XcnVWb+4ZP4Si8iqeyd5Im6QE/vHVNvYVV7ArvyyipNFkqtpiH1lZWRqp7Ozs\niI9tDhZf08RzfM0ZW0lFlVZU1WhZZbWuySkM6Zhw4stes0enf7E5othqanxaUFYZ9nHB8RWUVer9\nby/XFz7dpHO+3a15JRURxbIrv1TzSyvV5/PpjK+3am5heYP7r91dqP3unaWXPD3/kPK5q3frm7Pn\nqqrqXa8t1qlvLdc9BWXq8/kajWHb/hLdV9Tw66qqbswt0n73ztK3F29vdN+6ZGdnK7BQI/zctZqG\nMUextm0O/osP7t7+iJ9/3ODQRliuS0KCNPmu+w6pyTzynZObdA7gkHuirj21b6P7H5/ZnrW/GX9Y\n+XlDMpm32xuJ+g/XjQgrhlBrDT0z0rjghEw6twt/oNEjwZKGMcZEoL7ZNqMtNTmRF26JqDviiIir\nW0BFZLyIrBWRDSIyNdbxGGOMOVTcJA0RSQSeASYAQ4HrRWRow0cZY4xpTnGTNIDRwAZV3aSqlcAM\n4PIYx2SMMSZIPCWNXkDwWN47XJkxxpg4ETc394nIVcB4Vf2+W78JOE1Vf1xrvynAFIDMzMysGTNm\nRPR6xcXFpKfHduCvhlh8TRPP8cVzbGDxNVVLiG/SpEkR39wX83st/A/gdODDoPX7gPsaOsbu04gd\niy9y8RybqsXXVC0hPppwn0Y8NU99AwwSkWNFpA1wHTAzxjEZY4wJEjf3aahqtYj8GPgQSAReUtX6\nJ0EwxhjT7OKmTyMSIrIX2NrojnXrCuw7guEcaRZf08RzfPEcG1h8TdUS4munqg3P2FWPFp00mkJE\nFmqkHUHNwOJrmniOL55jA4uvqY72+OKpT8MYY0ycs6RhjDEmZK05aTwX6wAaYfE1TTzHF8+xgcXX\nVEd1fK22T8MYY0z4WnNNwxhjTJgsaRhjjAlZq0wa8TBvh4i8JCK5IrIyqKyziMwRkfXuuZMrFxF5\n2sW7XERGRjm2PiKSLSKrRORbEbkrzuJLFZGvRWSZi+/XrvxYEfnKxfG6G1kAEUlx6xvc9v7RjC8o\nzkQRWSIis+ItPhHZIiIrRGSpiCx0ZfHy+80QkTdFZI2IrBaR0+MotsHuZ+Z/FIrIT+MlPveaP3P/\nFytF5DX3/3Lk/vYiHX+kpT7w7jbfCAwA2gDLgKExiONsYCSwMqjscWCqW54K/NYtTwT+BQgwBvgq\nyrH1AEa65fbAOrw5TuIlPgHS3XIy8JV73TeA61z5X4AfuuUfAX9xy9cBrzfT7/hu4B/ALLceN/EB\nW4Cutcri5fc7Hfi+W24DZMRLbLXiTAR2A/3iJT68kcE3A2lBf3O3Hsm/vWb54cbTgwgGRoxiLP05\nNGmsBXq45R7AWrf8V+D6uvZrpjjfAy6Mx/iAtsBi4DS8u3CTav+e8YamOd0tJ7n9JMpx9QbmAucB\ns9yHRjzFt4XDk0bMf79AR/ehJ/EWWx2xXgR8Hk/xcXCKic7ub2kWcPGR/Ntrjc1T8TxvR6aq5rjl\n3UCmW45ZzK66OgLv23zcxOeafpYCucAcvNpjvqpW1xFDID63vQDoEs34gD8A9wA+t94lzuJT4N8i\nski86QYgPn6/xwJ7gb+5pr0XRKRdnMRW23XAa245LuJT1Z3A/wDbgBy8v6VFHMG/vdaYNFoE9VJ/\nTK+HFpF04C3gp6paGLwt1vGpao2qDsf7Rj8aGBKrWGoTkUuBXFVdFOtYGjBWVUfiTa98h4icHbwx\nhr/fJLxm22dVdQRQgtfcEw+xBbg+gcuA/6u9LZbxub6Uy/GSb0+gHTD+SL5Ga0waO4E+Qeu9XVk8\n2CMiPQDcc64rb/aYRSQZL2G8qqpvx1t8fqqaD2TjVbkzRMQ/cnNwDIH43PaOwP4ohnUmcJmIbMGb\ntvg84Kk4is//jRRVzQXewUu88fD73QHsUNWv3PqbeEkkHmILNgFYrKp73Hq8xHcBsFlV96pqFfA2\n3t/jEfvba41JI57n7ZgJ3OKWb8HrS/CX3+yuxBgDFARVhY84ERHgRWC1qj4Rh/F1E5EMt5yG19+y\nGi95XFVPfP64rwI+dt8Go0JV71PV3qraH+/v62NVvTFe4hORdiLS3r+M1za/kjj4/arqbmC7iAx2\nRecDq+Ihtlqu52DTlD+OeIhvGzBGRNq6/2P/z+/I/e01R4dRvD3wrmhYh9cO/kCMYngNr82xCu/b\n1WS8tsS5wHrgI6Cz21eAZ1y8K4BRUY5tLF71ejmw1D0mxlF8pwBLXHwrgV+58gHA18AGvGaDFFee\n6tY3uO0DmvH3PI6DV0/FRXwujmXu8a3/fyCOfr/DgYXu9/su0CleYnOv2Q7v23jHoLJ4iu/XwBr3\nv/EKkHIk//ZsGBFjjDEha43NU8YYYyJkScMYY0zILGkYY4wJmSUNY4wxIbOkYYwxJmSWNMxRQ0Qu\nk0ZGLRaRniLyplu+VUT+FOZr3B/CPn8Xkasa2y9aRGSeiIyK1eubo5slDXPUUNWZqjqtkX12qWpT\nPtAbTRotWdBdw8bUyZKGiXsi0l+8uRX+LiLrRORVEblARD538xeMdvsFag5u36dF5AsR2eT/5u/O\ntTLo9H3cN/P1IvJQ0Gu+6wbz+9Y/oJ+ITAPSxJtH4VVXdrN48yQsE5FXgs57du3XruM9rRaR591r\n/Nvd3X5ITUFEurrhSPzv713x5mvYIiI/FpG7xRvYb4GIdA56iZtcnCuDfj7txJvH5Wt3zOVB550p\nIh/j3aBmTL0saZiWYiDwe7yBCYcAN+Dduf5f1P/tv4fb51KgvhrIaOC7eHeZXx3UrPM9Vc0CRgF3\nikgXVZ0KlKnqcFW9UUROBH4JnKeqw4C7wnztQcAzqnoikO/iaMxJwJXAqcAjQKl6A/t9CdwctF9b\n9QZ0/BHwkit7AG+YiNHAucDv3DAi4I3vdJWqnhNCDKYVs6RhWorNqrpCVX14Q1/MVW84gxV485LU\n5V1V9anqKg4OVV3bHFXdr6pleIO7jXXld4rIMmAB3oBug+o49jzg/1R1H4CqHgjztTer6lK3vKiB\n9xEsW1WLVHUv3jDW/3TltX8Or7mY5gMd3FhdFwFTxRtSfh7eEBJ93f5zasVvTJ2s/dK0FBVBy76g\ndR/1/x0HHyP17FN7HB0VkXF4o4WerqqlIjIP7wM2HKG8dvA+NUCaW67m4Be62q8b6s/hsPfl4viu\nqq4N3iAip+ENQW5Mo6ymYVq7C8Wb3zkNuAL4HG946DyXMIbgTdPpVyXesPEAH+M1aXUBb47tIxTT\nFiDLLUfaaX8tgIiMxRtZtQBvlrafuNFPEZERTYzTtEKWNExr9zXevCHLgbdUdSHwAZAkIqvx+iMW\nBO3/HLBcRF5V1W/x+hU+cU1ZT3Bk/A/wQxFZAnSN8Bzl7vi/4I2gDPAw3pzqy0XkW7duTFhslFtj\njDEhs5qGMcaYkFnSMMYYEzJLGsYYY0JmScMYY0zILGkYY4wJmSUNY4wxIbOkYYwxJmT/HyZu5MiD\ni6yLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81cc55f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Overall loss = 0.445 and accuracy of 0.357\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,True)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def complex_model(X,y,is_training):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    mean1 = tf.get_variable(\"mean1\", shape= [32,])\n",
    "    var1 = tf.get_variable(\"var1\", shape = [32,])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 1024])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "    W2 = tf.get_variable(\"W2\", shape=[1024, 10])\n",
    "    b2 = tf.get_variable(\"b2\", shape=[10])\n",
    "    \n",
    "    offset = np.zeros_like(mean1)\n",
    "    scale = np.ones_like(var1)\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    # convolution\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding='VALID') + bconv1\n",
    "    #Relu\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    #Spatial batch normalization\n",
    "    h1b = tf.layers.batch_normalization(h1, training=is_training)\n",
    "    # Max pooling\n",
    "    h1p = tf.nn.max_pool(h1b,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    h1p_flat = tf.reshape(h1p,[-1,5408])\n",
    "    # Affine\n",
    "    X1 = tf.matmul(h1p_flat,W1) + b1\n",
    "    #Relu\n",
    "    X1R = tf.nn.relu(X1)\n",
    "    # Affine\n",
    "    y_out = tf.matmul(X1R,W2) + b2\n",
    "    return y_out\n",
    "\n",
    "y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 20.4 ms per loop\n",
      "(64, 10)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "session_conf = tf.ConfigProto(\n",
    "    device_count={'CPU' : 1, 'GPU' : 0},\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=False)\n",
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "x = np.random.randn(64, 32, 32,3)\n",
    "with tf.Session(config=session_conf) as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        print(ans.shape)\n",
    "        print(np.array_equal(ans.shape, np.array([64, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following from the run above \n",
    "\n",
    "`(64, 10)`\n",
    "\n",
    "`True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU!\n",
    "\n",
    "Now, we're going to try and start the model under the GPU device, the rest of the code stays unchanged and all our variables and operations will be computed using accelerated code paths. However, if there is no GPU, we get a Python exception and have to rebuild our graph. On a dual-core CPU, you might see around 50-80ms/batch running the above, while the Google Cloud GPUs (run below) should be around 2-5ms/batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.46 ms per loop\n"
     ]
    }
   ],
   "source": [
    "session_conf = tf.ConfigProto(\n",
    "    device_count={'CPU' : 1, 'GPU' : 1},\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=True)\n",
    "try:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        with tf.device(\"/gpu:0\") as dev: #\"/cpu:0\" or \"/gpu:0\"\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "            %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")    \n",
    "    # rebuild the graph\n",
    "    # trying to start a GPU throws an exception \n",
    "    # and also trashes the original graph\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that even a simple forward pass like this is significantly faster on the GPU. So for the rest of the assignment (and when you go train your models in assignment 3 and your project!), you should use GPU devices. However, with TensorFlow, the default device is a GPU if one is available, and a CPU otherwise, so we can skip the device specification from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.\n",
    "\n",
    "Now that you've seen how to define a model and do a single forward pass of some data through it, let's  walk through how you'd actually train one whole epoch over your training data (using the complex_model you created provided above).\n",
    "\n",
    "Make sure you understand how each TensorFlow function used below corresponds to what you implemented in your custom neural network implementation.\n",
    "\n",
    "First, set up an **RMSprop optimizer** (using a 1e-3 learning rate) and a **cross-entropy loss** function. See the TensorFlow documentation for more information\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "#     y_out: is what your model computes\n",
    "#     y: is your TensorFlow variable with label information\n",
    "# Outputs\n",
    "#    mean_loss: a TensorFlow variable (scalar) with numerical loss\n",
    "#    optimizer: a TensorFlow optimizer\n",
    "# This should be ~3 lines of code!\n",
    "\n",
    "# define the loss\n",
    "y_hot = tf.one_hot(y,10)\n",
    "mean_loss =  tf.losses.softmax_cross_entropy(y_hot,y_out) \n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(1e-3,epsilon=1e-3)\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Below we'll create a session and train the model over one epoch. You should see a loss of 1.4 to 2.0 and an accuracy of 0.4 to 0.5. There will be some variation due to random seeds and differences in initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 3.51 and accuracy of 0.031\n",
      "Iteration 100: with minibatch training loss = 1.91 and accuracy of 0.34\n",
      "Iteration 200: with minibatch training loss = 1.6 and accuracy of 0.48\n",
      "Iteration 300: with minibatch training loss = 1.3 and accuracy of 0.52\n",
      "Iteration 400: with minibatch training loss = 1.35 and accuracy of 0.5\n",
      "Iteration 500: with minibatch training loss = 1.35 and accuracy of 0.52\n",
      "Iteration 600: with minibatch training loss = 1.18 and accuracy of 0.55\n",
      "Iteration 700: with minibatch training loss = 1.4 and accuracy of 0.58\n",
      "Epoch 1, Overall loss = 1.47 and accuracy of 0.493\n",
      "Iteration 800: with minibatch training loss = 1.08 and accuracy of 0.58\n",
      "Iteration 900: with minibatch training loss = 1.19 and accuracy of 0.59\n",
      "Iteration 1000: with minibatch training loss = 1.2 and accuracy of 0.56\n",
      "Iteration 1100: with minibatch training loss = 0.841 and accuracy of 0.66\n",
      "Iteration 1200: with minibatch training loss = 0.843 and accuracy of 0.69\n",
      "Iteration 1300: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 1400: with minibatch training loss = 1.14 and accuracy of 0.64\n",
      "Iteration 1500: with minibatch training loss = 0.941 and accuracy of 0.66\n",
      "Epoch 2, Overall loss = 0.923 and accuracy of 0.683\n",
      "Iteration 1600: with minibatch training loss = 0.735 and accuracy of 0.72\n",
      "Iteration 1700: with minibatch training loss = 0.895 and accuracy of 0.66\n",
      "Iteration 1800: with minibatch training loss = 0.6 and accuracy of 0.77\n",
      "Iteration 1900: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 2000: with minibatch training loss = 0.559 and accuracy of 0.81\n",
      "Iteration 2100: with minibatch training loss = 0.624 and accuracy of 0.77\n",
      "Iteration 2200: with minibatch training loss = 0.579 and accuracy of 0.8\n",
      "Epoch 3, Overall loss = 0.665 and accuracy of 0.78\n",
      "Iteration 2300: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 2400: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 2500: with minibatch training loss = 0.481 and accuracy of 0.8\n",
      "Iteration 2600: with minibatch training loss = 0.42 and accuracy of 0.88\n",
      "Iteration 2700: with minibatch training loss = 0.492 and accuracy of 0.88\n",
      "Iteration 2800: with minibatch training loss = 0.457 and accuracy of 0.88\n",
      "Iteration 2900: with minibatch training loss = 0.396 and accuracy of 0.89\n",
      "Iteration 3000: with minibatch training loss = 0.242 and accuracy of 0.95\n",
      "Epoch 4, Overall loss = 0.439 and accuracy of 0.866\n",
      "Iteration 3100: with minibatch training loss = 0.357 and accuracy of 0.92\n",
      "Iteration 3200: with minibatch training loss = 0.325 and accuracy of 0.92\n",
      "Iteration 3300: with minibatch training loss = 0.239 and accuracy of 0.91\n",
      "Iteration 3400: with minibatch training loss = 0.212 and accuracy of 0.97\n",
      "Iteration 3500: with minibatch training loss = 0.233 and accuracy of 0.95\n",
      "Iteration 3600: with minibatch training loss = 0.163 and accuracy of 0.97\n",
      "Iteration 3700: with minibatch training loss = 0.219 and accuracy of 0.94\n",
      "Iteration 3800: with minibatch training loss = 0.169 and accuracy of 0.98\n",
      "Epoch 5, Overall loss = 0.246 and accuracy of 0.937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.24585313680463908,\n",
       " 0.93691836734693879,\n",
       " [0.49322448979591837,\n",
       "  0.68253061224489797,\n",
       "  0.78008163265306119,\n",
       "  0.86616326530612242,\n",
       "  0.93691836734693879],\n",
       " [0.58099999999999996,\n",
       "  0.63200000000000001,\n",
       "  0.63800000000000001,\n",
       "  0.624,\n",
       "  0.64800000000000002])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,5,64,100,train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Let's see the train and test code in action -- feel free to use these methods when evaluating the models you develop below. You should see a loss of 1.3 to 2.0 with an accuracy of 0.45 to 0.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation, N =  (1000,)\n",
      "Overall loss = 1.32 and accuracy of 0.648\n",
      "Test, N=  (10000,)\n",
      "Overall loss = 1.28 and accuracy of 0.642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2771272752761842,\n",
       " 0.64239999999999997,\n",
       " [0.64239999999999997],\n",
       " [0.64900000000000002])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validation, N = ', y_val.shape)\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n",
    "print ('Test, N= ', y_test.shape)\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)\n",
    "#print (tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves ** >= 70% accuracy on the validation set** of CIFAR-10. You can use the `run_model` function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use TensorFlow Scope**: Use TensorFlow scope and/or [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) to make it easier to write deeper networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for how to use `tf.layers`. \n",
    "- **Use Learning Rate Decay**: [As the notes point out](http://cs231n.github.io/neural-networks-3/#anneal), decaying the learning rate might help the model converge. Feel free to decay every epoch, when loss doesn't change over an entire epoch, or any other heuristic you find appropriate. See the [Tensorflow documentation](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) for learning rate decay.\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use [Dropout as in the TensorFlow MNIST tutorial](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and we'll save the test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at **>= 70% accuracy on the validation set**. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training and validation set accuracies for your final trained network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data\\\n",
    "        (num_training=49000, num_validation=1000, num_test=10000)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Adding 60 hidden convolution layers ***\n"
     ]
    }
   ],
   "source": [
    "from cs231n.my_tf_model import my_model1, my_model2, my_model3, my_model4\n",
    "# Feel free to play with this cell\n",
    "# Define model\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    # define our weights (e.g. init_two_layer_convnet)    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    mean1 = tf.get_variable(\"mean1\", shape= [32,])\n",
    "    var1 = tf.get_variable(\"var1\", shape = [32,])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 1024])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "    W2 = tf.get_variable(\"W2\", shape=[1024, 10])\n",
    "    b2 = tf.get_variable(\"b2\", shape=[10])\n",
    "    \n",
    "    offset = np.zeros_like(mean1)\n",
    "    scale = np.ones_like(var1)\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    # convolution\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding='VALID') + bconv1\n",
    "    #Relu\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    #Spatial batch normalization\n",
    "    h1b = tf.layers.batch_normalization(h1, training=is_training)\n",
    "    # Max pooling\n",
    "    h1p = tf.nn.max_pool(h1b,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    h1p_flat = tf.reshape(h1p,[-1,5408])\n",
    "    # Affine\n",
    "    X1 = tf.matmul(h1p_flat,W1) + b1\n",
    "    #Relu\n",
    "    X1R = tf.nn.relu(X1)\n",
    "    # Affine\n",
    "    y_out = tf.matmul(X1R,W2) + b2\n",
    "    return y_out \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setup the input variables (\"tf placeholders\")\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "#xx = prtx()\n",
    "y_out = my_model4(X,y,is_training)\n",
    "\n",
    "\n",
    "# Setup Loss function\n",
    "mean_loss =  tf.losses.softmax_cross_entropy(tf.one_hot(y,10),y_out) \n",
    "\n",
    "#Setup optimizer\n",
    "optimizer = tf.train.RMSPropOptimizer(1e-3,epsilon=1e-3) \n",
    "  \n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "saver = tf.train.Saver()\n",
    "i = 0\n",
    "\n",
    "# report how deep is the model...\n",
    "tvar = tf.trainable_variables()\n",
    "for var in tvar:\n",
    "    if (str(var).find('W:0') >= 0):\n",
    "        i +=1\n",
    "print ('*** Adding %d hidden convolution layers ***' % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 32, 32, 3)\n",
      "(49000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "X_train_f = np.flip(X_train,2)\n",
    "print (X_train_f.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Starting ***\n",
      "***Hyper iteration 0, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 47.3 and accuracy of 0.12\n",
      "Epoch 1, Overall loss = 4.01 and accuracy of 0.105\n",
      "Iteration 1000: with minibatch training loss = 2.3 and accuracy of 0.11\n",
      "Epoch 2, Overall loss = 2.31 and accuracy of 0.1\n",
      "Iteration 2000: with minibatch training loss = 2.3 and accuracy of 0.047\n",
      "Epoch 3, Overall loss = 2.31 and accuracy of 0.101\n",
      "Iteration 3000: with minibatch training loss = 2.31 and accuracy of 0.062\n",
      "Epoch 4, Overall loss = 2.3 and accuracy of 0.105\n",
      "Epoch 5, Overall loss = 2.28 and accuracy of 0.118\n",
      "Iteration 4000: with minibatch training loss = 2.16 and accuracy of 0.12\n",
      "Epoch 6, Overall loss = 2.27 and accuracy of 0.128\n",
      "Iteration 5000: with minibatch training loss = 2.26 and accuracy of 0.25\n",
      "Epoch 7, Overall loss = 2.24 and accuracy of 0.139\n",
      "Iteration 6000: with minibatch training loss = 2.32 and accuracy of 0.14\n",
      "Epoch 8, Overall loss = 2.22 and accuracy of 0.156\n",
      "Epoch 9, Overall loss = 2.18 and accuracy of 0.169\n",
      "Iteration 7000: with minibatch training loss = 2.11 and accuracy of 0.11\n",
      "Epoch 10, Overall loss = 2.16 and accuracy of 0.18\n",
      "*** Latest validation accuracy = 0.1779 ***\n",
      "***Hyper iteration 1, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 2.14 and accuracy of 0.094\n",
      "Epoch 11, Overall loss = 2.12 and accuracy of 0.193\n",
      "Iteration 1000: with minibatch training loss = 1.94 and accuracy of 0.2\n",
      "Epoch 12, Overall loss = 2.1 and accuracy of 0.204\n",
      "Iteration 2000: with minibatch training loss = 2.06 and accuracy of 0.22\n",
      "Epoch 13, Overall loss = 2.07 and accuracy of 0.213\n",
      "Iteration 3000: with minibatch training loss = 2.18 and accuracy of 0.25\n",
      "Epoch 14, Overall loss = 2.05 and accuracy of 0.221\n",
      "Epoch 15, Overall loss = 2.03 and accuracy of 0.233\n",
      "Iteration 4000: with minibatch training loss = 1.97 and accuracy of 0.23\n",
      "Epoch 16, Overall loss = 1.99 and accuracy of 0.25\n",
      "Iteration 5000: with minibatch training loss = 1.86 and accuracy of 0.25\n",
      "Epoch 17, Overall loss = 1.97 and accuracy of 0.258\n",
      "Iteration 6000: with minibatch training loss = 1.84 and accuracy of 0.28\n",
      "Epoch 18, Overall loss = 1.95 and accuracy of 0.263\n",
      "Epoch 19, Overall loss = 1.93 and accuracy of 0.272\n",
      "Iteration 7000: with minibatch training loss = 1.78 and accuracy of 0.3\n",
      "Epoch 20, Overall loss = 1.91 and accuracy of 0.278\n",
      "*** Latest validation accuracy = 0.3218 ***\n",
      "***Hyper iteration 2, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 2.1 and accuracy of 0.34\n",
      "Epoch 21, Overall loss = 1.89 and accuracy of 0.284\n",
      "Iteration 1000: with minibatch training loss = 1.95 and accuracy of 0.31\n",
      "Epoch 22, Overall loss = 1.87 and accuracy of 0.295\n",
      "Iteration 2000: with minibatch training loss = 2.19 and accuracy of 0.25\n",
      "Epoch 23, Overall loss = 1.84 and accuracy of 0.306\n",
      "Iteration 3000: with minibatch training loss = 2.09 and accuracy of 0.27\n",
      "Epoch 24, Overall loss = 1.8 and accuracy of 0.317\n",
      "Epoch 25, Overall loss = 1.79 and accuracy of 0.32\n",
      "Iteration 4000: with minibatch training loss = 1.65 and accuracy of 0.28\n",
      "Epoch 26, Overall loss = 1.78 and accuracy of 0.321\n",
      "Iteration 5000: with minibatch training loss = 1.82 and accuracy of 0.28\n",
      "Epoch 27, Overall loss = 1.76 and accuracy of 0.332\n",
      "Iteration 6000: with minibatch training loss = 1.84 and accuracy of 0.3\n",
      "Epoch 28, Overall loss = 1.74 and accuracy of 0.336\n",
      "Epoch 29, Overall loss = 1.72 and accuracy of 0.342\n",
      "Iteration 7000: with minibatch training loss = 1.76 and accuracy of 0.39\n",
      "Epoch 30, Overall loss = 1.7 and accuracy of 0.347\n",
      "*** Latest validation accuracy = 0.3737 ***\n",
      "***Hyper iteration 3, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 1.86 and accuracy of 0.3\n",
      "Epoch 31, Overall loss = 1.69 and accuracy of 0.353\n",
      "Iteration 1000: with minibatch training loss = 1.58 and accuracy of 0.3\n",
      "Epoch 32, Overall loss = 1.68 and accuracy of 0.357\n",
      "Iteration 2000: with minibatch training loss = 1.51 and accuracy of 0.44\n",
      "Epoch 33, Overall loss = 1.66 and accuracy of 0.362\n",
      "Iteration 3000: with minibatch training loss = 1.43 and accuracy of 0.52\n",
      "Epoch 34, Overall loss = 1.64 and accuracy of 0.367\n",
      "Epoch 35, Overall loss = 1.62 and accuracy of 0.371\n",
      "Iteration 4000: with minibatch training loss = 1.71 and accuracy of 0.33\n",
      "Epoch 36, Overall loss = 1.6 and accuracy of 0.378\n",
      "Iteration 5000: with minibatch training loss = 1.74 and accuracy of 0.28\n",
      "Epoch 37, Overall loss = 1.59 and accuracy of 0.379\n",
      "Iteration 6000: with minibatch training loss = 1.42 and accuracy of 0.36\n",
      "Epoch 38, Overall loss = 1.57 and accuracy of 0.388\n",
      "Epoch 39, Overall loss = 1.56 and accuracy of 0.39\n",
      "Iteration 7000: with minibatch training loss = 1.92 and accuracy of 0.44\n",
      "Epoch 40, Overall loss = 1.55 and accuracy of 0.393\n",
      "*** Latest validation accuracy = 0.435 ***\n",
      "***Hyper iteration 4, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 1.56 and accuracy of 0.42\n",
      "Epoch 41, Overall loss = 1.55 and accuracy of 0.392\n",
      "Iteration 1000: with minibatch training loss = 1.41 and accuracy of 0.36\n",
      "Epoch 42, Overall loss = 1.54 and accuracy of 0.396\n",
      "Iteration 2000: with minibatch training loss = 1.56 and accuracy of 0.42\n",
      "Epoch 43, Overall loss = 1.53 and accuracy of 0.4\n",
      "Iteration 3000: with minibatch training loss = 1.61 and accuracy of 0.41\n",
      "Epoch 44, Overall loss = 1.52 and accuracy of 0.407\n",
      "Epoch 45, Overall loss = 1.51 and accuracy of 0.408\n",
      "Iteration 4000: with minibatch training loss = 1.35 and accuracy of 0.38\n",
      "Epoch 46, Overall loss = 1.5 and accuracy of 0.411\n",
      "Iteration 5000: with minibatch training loss = 1.56 and accuracy of 0.41\n",
      "Epoch 47, Overall loss = 1.49 and accuracy of 0.414\n",
      "Iteration 6000: with minibatch training loss = 1.9 and accuracy of 0.41\n",
      "Epoch 48, Overall loss = 1.47 and accuracy of 0.419\n",
      "Epoch 49, Overall loss = 1.45 and accuracy of 0.425\n",
      "Iteration 7000: with minibatch training loss = 1.37 and accuracy of 0.45\n",
      "Epoch 50, Overall loss = 1.45 and accuracy of 0.427\n",
      "*** Latest validation accuracy = 0.4573 ***\n",
      "***Hyper iteration 5, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 1.81 and accuracy of 0.3\n",
      "Epoch 51, Overall loss = 1.46 and accuracy of 0.43\n",
      "Iteration 1000: with minibatch training loss = 1.22 and accuracy of 0.42\n",
      "Epoch 52, Overall loss = 1.43 and accuracy of 0.438\n",
      "Iteration 2000: with minibatch training loss = 1.31 and accuracy of 0.45\n",
      "Epoch 53, Overall loss = 1.43 and accuracy of 0.443\n",
      "Iteration 3000: with minibatch training loss = 1.37 and accuracy of 0.44\n",
      "Epoch 54, Overall loss = 1.41 and accuracy of 0.45\n",
      "Epoch 55, Overall loss = 1.38 and accuracy of 0.463\n",
      "Iteration 4000: with minibatch training loss = 1.57 and accuracy of 0.45\n",
      "Epoch 56, Overall loss = 1.36 and accuracy of 0.475\n",
      "Iteration 5000: with minibatch training loss = 1.29 and accuracy of 0.52\n",
      "Epoch 57, Overall loss = 1.34 and accuracy of 0.488\n",
      "Iteration 6000: with minibatch training loss = 1.13 and accuracy of 0.56\n",
      "Epoch 58, Overall loss = 1.33 and accuracy of 0.49\n",
      "Epoch 59, Overall loss = 1.32 and accuracy of 0.495\n",
      "Iteration 7000: with minibatch training loss = 1.33 and accuracy of 0.42\n",
      "Epoch 60, Overall loss = 1.3 and accuracy of 0.504\n",
      "*** Latest validation accuracy = 0.5206 ***\n",
      "***Hyper iteration 6, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 1.29 and accuracy of 0.5\n",
      "Epoch 61, Overall loss = 1.3 and accuracy of 0.506\n",
      "Iteration 1000: with minibatch training loss = 1.22 and accuracy of 0.52\n",
      "Epoch 62, Overall loss = 1.29 and accuracy of 0.51\n",
      "Iteration 2000: with minibatch training loss = 1.55 and accuracy of 0.44\n",
      "Epoch 63, Overall loss = 1.27 and accuracy of 0.519\n",
      "Iteration 3000: with minibatch training loss = 1.07 and accuracy of 0.62\n",
      "Epoch 64, Overall loss = 1.24 and accuracy of 0.531\n",
      "Epoch 65, Overall loss = 1.22 and accuracy of 0.538\n",
      "Iteration 4000: with minibatch training loss = 1.1 and accuracy of 0.61\n",
      "Epoch 66, Overall loss = 1.2 and accuracy of 0.545\n",
      "Iteration 5000: with minibatch training loss = 0.991 and accuracy of 0.64\n",
      "Epoch 67, Overall loss = 1.19 and accuracy of 0.553\n",
      "Iteration 6000: with minibatch training loss = 1.01 and accuracy of 0.53\n",
      "Epoch 68, Overall loss = 1.18 and accuracy of 0.559\n",
      "Epoch 69, Overall loss = 1.16 and accuracy of 0.566\n",
      "Iteration 7000: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Epoch 70, Overall loss = 1.14 and accuracy of 0.572\n",
      "*** Latest validation accuracy = 0.589 ***\n",
      "***Hyper iteration 7, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 1.37 and accuracy of 0.52\n",
      "Epoch 71, Overall loss = 1.17 and accuracy of 0.562\n",
      "Iteration 1000: with minibatch training loss = 0.88 and accuracy of 0.66\n",
      "Epoch 72, Overall loss = 1.14 and accuracy of 0.574\n",
      "Iteration 2000: with minibatch training loss = 0.933 and accuracy of 0.62\n",
      "Epoch 73, Overall loss = 1.12 and accuracy of 0.586\n",
      "Iteration 3000: with minibatch training loss = 1.32 and accuracy of 0.45\n",
      "Epoch 74, Overall loss = 1.1 and accuracy of 0.597\n",
      "Epoch 75, Overall loss = 1.09 and accuracy of 0.606\n",
      "Iteration 4000: with minibatch training loss = 0.93 and accuracy of 0.58\n",
      "Epoch 76, Overall loss = 1.06 and accuracy of 0.621\n",
      "Iteration 5000: with minibatch training loss = 0.796 and accuracy of 0.73\n",
      "Epoch 77, Overall loss = 1.05 and accuracy of 0.629\n",
      "Iteration 6000: with minibatch training loss = 1.04 and accuracy of 0.55\n",
      "Epoch 78, Overall loss = 1.02 and accuracy of 0.639\n",
      "Epoch 79, Overall loss = 1 and accuracy of 0.648\n",
      "Iteration 7000: with minibatch training loss = 1.1 and accuracy of 0.64\n",
      "Epoch 80, Overall loss = 0.994 and accuracy of 0.651\n",
      "*** Latest validation accuracy = 0.6454 ***\n",
      "***Hyper iteration 8, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.998 and accuracy of 0.64\n",
      "Epoch 81, Overall loss = 1.01 and accuracy of 0.652\n",
      "Iteration 1000: with minibatch training loss = 0.765 and accuracy of 0.66\n",
      "Epoch 82, Overall loss = 0.995 and accuracy of 0.659\n",
      "Iteration 2000: with minibatch training loss = 0.79 and accuracy of 0.66\n",
      "Epoch 83, Overall loss = 0.956 and accuracy of 0.668\n",
      "Iteration 3000: with minibatch training loss = 0.918 and accuracy of 0.66\n",
      "Epoch 84, Overall loss = 0.929 and accuracy of 0.678\n",
      "Epoch 85, Overall loss = 0.936 and accuracy of 0.677\n",
      "Iteration 4000: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Epoch 86, Overall loss = 0.899 and accuracy of 0.689\n",
      "Iteration 5000: with minibatch training loss = 0.747 and accuracy of 0.73\n",
      "Epoch 87, Overall loss = 0.891 and accuracy of 0.692\n",
      "Iteration 6000: with minibatch training loss = 0.854 and accuracy of 0.72\n",
      "Epoch 88, Overall loss = 0.865 and accuracy of 0.7\n",
      "Epoch 89, Overall loss = 0.865 and accuracy of 0.7\n",
      "Iteration 7000: with minibatch training loss = 0.719 and accuracy of 0.69\n",
      "Epoch 90, Overall loss = 0.856 and accuracy of 0.706\n",
      "*** Latest validation accuracy = 0.6984 ***\n",
      "***Hyper iteration 9, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.993 and accuracy of 0.69\n",
      "Epoch 91, Overall loss = 0.886 and accuracy of 0.697\n",
      "Iteration 1000: with minibatch training loss = 0.722 and accuracy of 0.72\n",
      "Epoch 92, Overall loss = 0.844 and accuracy of 0.711\n",
      "Iteration 2000: with minibatch training loss = 0.633 and accuracy of 0.77\n",
      "Epoch 93, Overall loss = 0.841 and accuracy of 0.71\n",
      "Iteration 3000: with minibatch training loss = 0.823 and accuracy of 0.75\n",
      "Epoch 94, Overall loss = 0.817 and accuracy of 0.717\n",
      "Epoch 95, Overall loss = 0.805 and accuracy of 0.721\n",
      "Iteration 4000: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Epoch 96, Overall loss = 0.802 and accuracy of 0.724\n",
      "Iteration 5000: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Epoch 97, Overall loss = 0.786 and accuracy of 0.729\n",
      "Iteration 6000: with minibatch training loss = 0.855 and accuracy of 0.7\n",
      "Epoch 98, Overall loss = 0.766 and accuracy of 0.734\n",
      "Epoch 99, Overall loss = 0.749 and accuracy of 0.738\n",
      "Iteration 7000: with minibatch training loss = 0.686 and accuracy of 0.73\n",
      "Epoch 100, Overall loss = 0.739 and accuracy of 0.74\n",
      "*** Latest validation accuracy = 0.7001 ***\n",
      "***Hyper iteration 10, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.695 and accuracy of 0.72\n",
      "Epoch 101, Overall loss = 0.796 and accuracy of 0.725\n",
      "Iteration 1000: with minibatch training loss = 0.682 and accuracy of 0.72\n",
      "Epoch 102, Overall loss = 0.754 and accuracy of 0.74\n",
      "Iteration 2000: with minibatch training loss = 0.728 and accuracy of 0.75\n",
      "Epoch 103, Overall loss = 0.745 and accuracy of 0.743\n",
      "Iteration 3000: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Epoch 104, Overall loss = 0.722 and accuracy of 0.747\n",
      "Epoch 105, Overall loss = 0.714 and accuracy of 0.752\n",
      "Iteration 4000: with minibatch training loss = 0.681 and accuracy of 0.75\n",
      "Epoch 106, Overall loss = 0.711 and accuracy of 0.75\n",
      "Iteration 5000: with minibatch training loss = 0.707 and accuracy of 0.75\n",
      "Epoch 107, Overall loss = 0.699 and accuracy of 0.757\n",
      "Iteration 6000: with minibatch training loss = 0.611 and accuracy of 0.77\n",
      "Epoch 108, Overall loss = 0.676 and accuracy of 0.763\n",
      "Epoch 109, Overall loss = 0.671 and accuracy of 0.765\n",
      "Iteration 7000: with minibatch training loss = 0.786 and accuracy of 0.72\n",
      "Epoch 110, Overall loss = 0.663 and accuracy of 0.767\n",
      "*** Latest validation accuracy = 0.7231 ***\n",
      "***Hyper iteration 11, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 1.72 and accuracy of 0.67\n",
      "Epoch 111, Overall loss = 0.693 and accuracy of 0.762\n",
      "Iteration 1000: with minibatch training loss = 0.585 and accuracy of 0.77\n",
      "Epoch 112, Overall loss = 0.683 and accuracy of 0.765\n",
      "Iteration 2000: with minibatch training loss = 0.458 and accuracy of 0.84\n",
      "Epoch 113, Overall loss = 0.667 and accuracy of 0.769\n",
      "Iteration 3000: with minibatch training loss = 0.649 and accuracy of 0.73\n",
      "Epoch 114, Overall loss = 0.652 and accuracy of 0.772\n",
      "Epoch 115, Overall loss = 0.638 and accuracy of 0.777\n",
      "Iteration 4000: with minibatch training loss = 0.464 and accuracy of 0.77\n",
      "Epoch 116, Overall loss = 0.631 and accuracy of 0.78\n",
      "Iteration 5000: with minibatch training loss = 0.492 and accuracy of 0.78\n",
      "Epoch 117, Overall loss = 0.619 and accuracy of 0.782\n",
      "Iteration 6000: with minibatch training loss = 0.53 and accuracy of 0.86\n",
      "Epoch 118, Overall loss = 0.604 and accuracy of 0.787\n",
      "Epoch 119, Overall loss = 0.601 and accuracy of 0.79\n",
      "Iteration 7000: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Epoch 120, Overall loss = 0.582 and accuracy of 0.796\n",
      "*** Latest validation accuracy = 0.7334 ***\n",
      "***Hyper iteration 12, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.623 and accuracy of 0.73\n",
      "Epoch 121, Overall loss = 0.639 and accuracy of 0.783\n",
      "Iteration 1000: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Epoch 122, Overall loss = 0.61 and accuracy of 0.791\n",
      "Iteration 2000: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Epoch 123, Overall loss = 0.595 and accuracy of 0.794\n",
      "Iteration 3000: with minibatch training loss = 0.371 and accuracy of 0.83\n",
      "Epoch 124, Overall loss = 0.583 and accuracy of 0.797\n",
      "Epoch 125, Overall loss = 0.575 and accuracy of 0.801\n",
      "Iteration 4000: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Epoch 126, Overall loss = 0.556 and accuracy of 0.806\n",
      "Iteration 5000: with minibatch training loss = 0.647 and accuracy of 0.83\n",
      "Epoch 127, Overall loss = 0.567 and accuracy of 0.805\n",
      "Iteration 6000: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Epoch 128, Overall loss = 0.543 and accuracy of 0.81\n",
      "Epoch 129, Overall loss = 0.536 and accuracy of 0.814\n",
      "Iteration 7000: with minibatch training loss = 0.8 and accuracy of 0.81\n",
      "Epoch 130, Overall loss = 0.521 and accuracy of 0.818\n",
      "*** Latest validation accuracy = 0.7421 ***\n",
      "***Hyper iteration 13, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.47 and accuracy of 0.81\n",
      "Epoch 131, Overall loss = 0.553 and accuracy of 0.81\n",
      "Iteration 1000: with minibatch training loss = 0.752 and accuracy of 0.86\n",
      "Epoch 132, Overall loss = 0.555 and accuracy of 0.808\n",
      "Iteration 2000: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Epoch 133, Overall loss = 0.537 and accuracy of 0.813\n",
      "Iteration 3000: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Epoch 134, Overall loss = 0.528 and accuracy of 0.816\n",
      "Epoch 135, Overall loss = 0.514 and accuracy of 0.82\n",
      "Iteration 4000: with minibatch training loss = 0.53 and accuracy of 0.8\n",
      "Epoch 136, Overall loss = 0.513 and accuracy of 0.822\n",
      "Iteration 5000: with minibatch training loss = 0.543 and accuracy of 0.77\n",
      "Epoch 137, Overall loss = 0.504 and accuracy of 0.826\n",
      "Iteration 6000: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Epoch 138, Overall loss = 0.5 and accuracy of 0.828\n",
      "Epoch 139, Overall loss = 0.485 and accuracy of 0.83\n",
      "Iteration 7000: with minibatch training loss = 0.367 and accuracy of 0.83\n",
      "Epoch 140, Overall loss = 0.479 and accuracy of 0.833\n",
      "*** Latest validation accuracy = 0.7503 ***\n",
      "***Hyper iteration 14, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Epoch 141, Overall loss = 0.513 and accuracy of 0.825\n",
      "Iteration 1000: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Epoch 142, Overall loss = 0.49 and accuracy of 0.832\n",
      "Iteration 2000: with minibatch training loss = 0.727 and accuracy of 0.75\n",
      "Epoch 143, Overall loss = 0.48 and accuracy of 0.835\n",
      "Iteration 3000: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Epoch 144, Overall loss = 0.47 and accuracy of 0.839\n",
      "Epoch 145, Overall loss = 0.466 and accuracy of 0.839\n",
      "Iteration 4000: with minibatch training loss = 0.803 and accuracy of 0.81\n",
      "Epoch 146, Overall loss = 0.462 and accuracy of 0.84\n",
      "Iteration 5000: with minibatch training loss = 0.421 and accuracy of 0.89\n",
      "Epoch 147, Overall loss = 0.442 and accuracy of 0.844\n",
      "Iteration 6000: with minibatch training loss = 0.51 and accuracy of 0.88\n",
      "Epoch 148, Overall loss = 0.448 and accuracy of 0.844\n",
      "Epoch 149, Overall loss = 0.439 and accuracy of 0.846\n",
      "Iteration 7000: with minibatch training loss = 0.383 and accuracy of 0.83\n",
      "Epoch 150, Overall loss = 0.43 and accuracy of 0.851\n",
      "*** Latest validation accuracy = 0.7534 ***\n",
      "***Hyper iteration 15, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.421 and accuracy of 0.89\n",
      "Epoch 151, Overall loss = 0.462 and accuracy of 0.841\n",
      "Iteration 1000: with minibatch training loss = 0.422 and accuracy of 0.91\n",
      "Epoch 152, Overall loss = 0.451 and accuracy of 0.845\n",
      "Iteration 2000: with minibatch training loss = 0.455 and accuracy of 0.73\n",
      "Epoch 153, Overall loss = 0.45 and accuracy of 0.847\n",
      "Iteration 3000: with minibatch training loss = 0.414 and accuracy of 0.86\n",
      "Epoch 154, Overall loss = 0.43 and accuracy of 0.854\n",
      "Epoch 155, Overall loss = 0.42 and accuracy of 0.856\n",
      "Iteration 4000: with minibatch training loss = 0.294 and accuracy of 0.88\n",
      "Epoch 156, Overall loss = 0.416 and accuracy of 0.858\n",
      "Iteration 5000: with minibatch training loss = 0.296 and accuracy of 0.89\n",
      "Epoch 157, Overall loss = 0.418 and accuracy of 0.858\n",
      "Iteration 6000: with minibatch training loss = 0.395 and accuracy of 0.86\n",
      "Epoch 158, Overall loss = 0.402 and accuracy of 0.862\n",
      "Epoch 159, Overall loss = 0.396 and accuracy of 0.863\n",
      "Iteration 7000: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Epoch 160, Overall loss = 0.403 and accuracy of 0.863\n",
      "*** Latest validation accuracy = 0.7545 ***\n",
      "***Hyper iteration 16, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.624 and accuracy of 0.89\n",
      "Epoch 161, Overall loss = 0.437 and accuracy of 0.855\n",
      "Iteration 1000: with minibatch training loss = 0.43 and accuracy of 0.86\n",
      "Epoch 162, Overall loss = 0.422 and accuracy of 0.858\n",
      "Iteration 2000: with minibatch training loss = 0.503 and accuracy of 0.83\n",
      "Epoch 163, Overall loss = 0.412 and accuracy of 0.862\n",
      "Iteration 3000: with minibatch training loss = 0.317 and accuracy of 0.88\n",
      "Epoch 164, Overall loss = 0.393 and accuracy of 0.867\n",
      "Epoch 165, Overall loss = 0.388 and accuracy of 0.869\n",
      "Iteration 4000: with minibatch training loss = 0.382 and accuracy of 0.81\n",
      "Epoch 166, Overall loss = 0.374 and accuracy of 0.873\n",
      "Iteration 5000: with minibatch training loss = 0.583 and accuracy of 0.8\n",
      "Epoch 167, Overall loss = 0.372 and accuracy of 0.875\n",
      "Iteration 6000: with minibatch training loss = 0.211 and accuracy of 0.91\n",
      "Epoch 168, Overall loss = 0.361 and accuracy of 0.878\n",
      "Epoch 169, Overall loss = 0.378 and accuracy of 0.874\n",
      "Iteration 7000: with minibatch training loss = 0.427 and accuracy of 0.91\n",
      "Epoch 170, Overall loss = 0.356 and accuracy of 0.88\n",
      "*** Latest validation accuracy = 0.7684 ***\n",
      "***Hyper iteration 17, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.369 and accuracy of 0.86\n",
      "Epoch 171, Overall loss = 0.4 and accuracy of 0.87\n",
      "Iteration 1000: with minibatch training loss = 0.314 and accuracy of 0.81\n",
      "Epoch 172, Overall loss = 0.392 and accuracy of 0.869\n",
      "Iteration 2000: with minibatch training loss = 0.204 and accuracy of 0.89\n",
      "Epoch 173, Overall loss = 0.371 and accuracy of 0.876\n",
      "Iteration 3000: with minibatch training loss = 0.444 and accuracy of 0.83\n",
      "Epoch 174, Overall loss = 0.364 and accuracy of 0.88\n",
      "Epoch 175, Overall loss = 0.361 and accuracy of 0.879\n",
      "Iteration 4000: with minibatch training loss = 0.255 and accuracy of 0.91\n",
      "Epoch 176, Overall loss = 0.342 and accuracy of 0.885\n",
      "Iteration 5000: with minibatch training loss = 0.411 and accuracy of 0.84\n",
      "Epoch 177, Overall loss = 0.341 and accuracy of 0.886\n",
      "Iteration 6000: with minibatch training loss = 0.328 and accuracy of 0.92\n",
      "Epoch 178, Overall loss = 0.337 and accuracy of 0.889\n",
      "Epoch 179, Overall loss = 0.326 and accuracy of 0.893\n",
      "Iteration 7000: with minibatch training loss = 0.326 and accuracy of 0.91\n",
      "Epoch 180, Overall loss = 0.32 and accuracy of 0.893\n",
      "*** Latest validation accuracy = 0.7708 ***\n",
      "***Hyper iteration 18, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.139 and accuracy of 0.95\n",
      "Epoch 181, Overall loss = 0.347 and accuracy of 0.888\n",
      "Iteration 1000: with minibatch training loss = 0.327 and accuracy of 0.89\n",
      "Epoch 182, Overall loss = 0.345 and accuracy of 0.888\n",
      "Iteration 2000: with minibatch training loss = 0.322 and accuracy of 0.91\n",
      "Epoch 183, Overall loss = 0.336 and accuracy of 0.89\n",
      "Iteration 3000: with minibatch training loss = 0.24 and accuracy of 0.94\n",
      "Epoch 184, Overall loss = 0.329 and accuracy of 0.893\n",
      "Epoch 185, Overall loss = 0.325 and accuracy of 0.894\n",
      "Iteration 4000: with minibatch training loss = 0.209 and accuracy of 0.92\n",
      "Epoch 186, Overall loss = 0.32 and accuracy of 0.895\n",
      "Iteration 5000: with minibatch training loss = 0.566 and accuracy of 0.88\n",
      "Epoch 187, Overall loss = 0.322 and accuracy of 0.896\n",
      "Iteration 6000: with minibatch training loss = 0.279 and accuracy of 0.94\n",
      "Epoch 188, Overall loss = 0.322 and accuracy of 0.895\n",
      "Epoch 189, Overall loss = 0.307 and accuracy of 0.901\n",
      "Iteration 7000: with minibatch training loss = 0.305 and accuracy of 0.86\n",
      "Epoch 190, Overall loss = 0.3 and accuracy of 0.904\n",
      "*** Latest validation accuracy = 0.7772 ***\n",
      "***Hyper iteration 19, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.221 and accuracy of 0.89\n",
      "Epoch 191, Overall loss = 0.331 and accuracy of 0.895\n",
      "Iteration 1000: with minibatch training loss = 0.399 and accuracy of 0.91\n",
      "Epoch 192, Overall loss = 0.326 and accuracy of 0.897\n",
      "Iteration 2000: with minibatch training loss = 0.265 and accuracy of 0.92\n",
      "Epoch 193, Overall loss = 0.312 and accuracy of 0.902\n",
      "Iteration 3000: with minibatch training loss = 0.506 and accuracy of 0.8\n",
      "Epoch 194, Overall loss = 0.3 and accuracy of 0.903\n",
      "Epoch 195, Overall loss = 0.291 and accuracy of 0.907\n",
      "Iteration 4000: with minibatch training loss = 0.342 and accuracy of 0.84\n",
      "Epoch 196, Overall loss = 0.32 and accuracy of 0.902\n",
      "Iteration 5000: with minibatch training loss = 0.288 and accuracy of 0.91\n",
      "Epoch 197, Overall loss = 0.295 and accuracy of 0.906\n",
      "Iteration 6000: with minibatch training loss = 0.278 and accuracy of 0.89\n",
      "Epoch 198, Overall loss = 0.289 and accuracy of 0.908\n",
      "Epoch 199, Overall loss = 0.272 and accuracy of 0.913\n",
      "Iteration 7000: with minibatch training loss = 0.336 and accuracy of 0.88\n",
      "Epoch 200, Overall loss = 0.28 and accuracy of 0.911\n",
      "*** Latest validation accuracy = 0.7794 ***\n",
      "***Hyper iteration 20, Learning rate = 0.1, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.43 and accuracy of 0.86\n",
      "Epoch 201, Overall loss = 0.31 and accuracy of 0.903\n",
      "Iteration 1000: with minibatch training loss = 0.181 and accuracy of 0.91\n",
      "Epoch 202, Overall loss = 0.303 and accuracy of 0.904\n",
      "Iteration 2000: with minibatch training loss = 0.0808 and accuracy of 0.95\n",
      "Epoch 203, Overall loss = 0.284 and accuracy of 0.91\n",
      "Iteration 3000: with minibatch training loss = 0.125 and accuracy of 0.95\n",
      "Epoch 204, Overall loss = 0.285 and accuracy of 0.914\n",
      "Epoch 205, Overall loss = 0.281 and accuracy of 0.914\n",
      "Iteration 4000: with minibatch training loss = 0.3 and accuracy of 0.92\n",
      "Epoch 206, Overall loss = 0.272 and accuracy of 0.915\n",
      "Iteration 5000: with minibatch training loss = 0.231 and accuracy of 0.91\n",
      "Epoch 207, Overall loss = 0.27 and accuracy of 0.916\n",
      "Iteration 6000: with minibatch training loss = 0.154 and accuracy of 0.95\n",
      "Epoch 208, Overall loss = 0.251 and accuracy of 0.921\n",
      "Epoch 209, Overall loss = 0.274 and accuracy of 0.917\n",
      "Iteration 7000: with minibatch training loss = 0.405 and accuracy of 0.89\n",
      "Epoch 210, Overall loss = 0.257 and accuracy of 0.921\n",
      "*** Latest validation accuracy = 0.7739 ***\n",
      "*** reducing Learning rate to 0.010000000000000002\n",
      "***Hyper iteration 21, Learning rate = 0.010000000000000002, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.191 and accuracy of 0.91\n",
      "Epoch 211, Overall loss = 0.331 and accuracy of 0.905\n",
      "Iteration 1000: with minibatch training loss = 0.324 and accuracy of 0.92\n",
      "Epoch 212, Overall loss = 0.291 and accuracy of 0.91\n",
      "Iteration 2000: with minibatch training loss = 0.146 and accuracy of 0.95\n",
      "Epoch 213, Overall loss = 0.27 and accuracy of 0.918\n",
      "Iteration 3000: with minibatch training loss = 0.335 and accuracy of 0.91\n",
      "Epoch 214, Overall loss = 0.26 and accuracy of 0.921\n",
      "Epoch 215, Overall loss = 0.278 and accuracy of 0.918\n",
      "Iteration 4000: with minibatch training loss = 0.325 and accuracy of 0.92\n",
      "Epoch 216, Overall loss = 0.266 and accuracy of 0.92\n",
      "Iteration 5000: with minibatch training loss = 0.291 and accuracy of 0.94\n",
      "Epoch 217, Overall loss = 0.249 and accuracy of 0.925\n",
      "Iteration 6000: with minibatch training loss = 0.171 and accuracy of 0.94\n",
      "Epoch 218, Overall loss = 0.244 and accuracy of 0.925\n",
      "Epoch 219, Overall loss = 0.23 and accuracy of 0.929\n",
      "Iteration 7000: with minibatch training loss = 0.62 and accuracy of 0.88\n",
      "Epoch 220, Overall loss = 0.231 and accuracy of 0.93\n",
      "*** Latest validation accuracy = 0.7802 ***\n",
      "*** reducing Learning rate to 0.0010000000000000002\n",
      "***Hyper iteration 22, Learning rate = 0.0010000000000000002, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.113 and accuracy of 0.97\n",
      "Epoch 221, Overall loss = 0.263 and accuracy of 0.922\n",
      "Iteration 1000: with minibatch training loss = 0.209 and accuracy of 0.94\n",
      "Epoch 222, Overall loss = 0.242 and accuracy of 0.927\n",
      "Iteration 2000: with minibatch training loss = 0.223 and accuracy of 0.94\n",
      "Epoch 223, Overall loss = 0.24 and accuracy of 0.93\n",
      "Iteration 3000: with minibatch training loss = 0.0678 and accuracy of 0.97\n",
      "Epoch 224, Overall loss = 0.239 and accuracy of 0.929\n",
      "Epoch 225, Overall loss = 0.229 and accuracy of 0.933\n",
      "Iteration 4000: with minibatch training loss = 0.171 and accuracy of 0.97\n",
      "Epoch 226, Overall loss = 0.223 and accuracy of 0.933\n",
      "Iteration 5000: with minibatch training loss = 0.195 and accuracy of 0.94\n",
      "Epoch 227, Overall loss = 0.222 and accuracy of 0.933\n",
      "Iteration 6000: with minibatch training loss = 0.179 and accuracy of 0.95\n",
      "Epoch 228, Overall loss = 0.216 and accuracy of 0.936\n",
      "Epoch 229, Overall loss = 0.227 and accuracy of 0.935\n",
      "Iteration 7000: with minibatch training loss = 0.306 and accuracy of 0.89\n",
      "Epoch 230, Overall loss = 0.226 and accuracy of 0.934\n",
      "*** Latest validation accuracy = 0.7799 ***\n",
      "*** reducing Learning rate to 0.00010000000000000003\n",
      "***Hyper iteration 23, Learning rate = 0.00010000000000000003, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.334 and accuracy of 0.89\n",
      "Epoch 231, Overall loss = 0.242 and accuracy of 0.931\n",
      "Iteration 1000: with minibatch training loss = 0.107 and accuracy of 0.95\n",
      "Epoch 232, Overall loss = 0.235 and accuracy of 0.935\n",
      "Iteration 2000: with minibatch training loss = 0.446 and accuracy of 0.89\n",
      "Epoch 233, Overall loss = 0.221 and accuracy of 0.935\n",
      "Iteration 3000: with minibatch training loss = 0.263 and accuracy of 0.89\n",
      "Epoch 234, Overall loss = 0.223 and accuracy of 0.936\n",
      "Epoch 235, Overall loss = 0.21 and accuracy of 0.937\n",
      "Iteration 4000: with minibatch training loss = 0.14 and accuracy of 0.97\n",
      "Epoch 236, Overall loss = 0.2 and accuracy of 0.942\n",
      "Iteration 5000: with minibatch training loss = 0.0771 and accuracy of 0.97\n",
      "Epoch 237, Overall loss = 0.198 and accuracy of 0.943\n",
      "Iteration 6000: with minibatch training loss = 0.15 and accuracy of 0.94\n",
      "Epoch 238, Overall loss = 0.19 and accuracy of 0.944\n",
      "Epoch 239, Overall loss = 0.19 and accuracy of 0.944\n",
      "Iteration 7000: with minibatch training loss = 0.0375 and accuracy of 1\n",
      "Epoch 240, Overall loss = 0.2 and accuracy of 0.943\n",
      "*** Latest validation accuracy = 0.7886 ***\n",
      "***Hyper iteration 24, Learning rate = 0.00010000000000000003, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.384 and accuracy of 0.95\n",
      "Epoch 241, Overall loss = 0.244 and accuracy of 0.934\n",
      "Iteration 1000: with minibatch training loss = 0.347 and accuracy of 0.91\n",
      "Epoch 242, Overall loss = 0.235 and accuracy of 0.936\n",
      "Iteration 2000: with minibatch training loss = 0.291 and accuracy of 0.92\n",
      "Epoch 243, Overall loss = 0.207 and accuracy of 0.941\n",
      "Iteration 3000: with minibatch training loss = 0.16 and accuracy of 0.94\n",
      "Epoch 244, Overall loss = 0.2 and accuracy of 0.943\n",
      "Epoch 245, Overall loss = 0.194 and accuracy of 0.945\n",
      "Iteration 4000: with minibatch training loss = 0.0432 and accuracy of 1\n",
      "Epoch 246, Overall loss = 0.223 and accuracy of 0.944\n",
      "Iteration 5000: with minibatch training loss = 0.215 and accuracy of 0.95\n",
      "Epoch 247, Overall loss = 0.187 and accuracy of 0.948\n",
      "Iteration 6000: with minibatch training loss = 0.0898 and accuracy of 0.98\n",
      "Epoch 248, Overall loss = 0.185 and accuracy of 0.949\n",
      "Epoch 249, Overall loss = 0.176 and accuracy of 0.95\n",
      "Iteration 7000: with minibatch training loss = 0.0963 and accuracy of 0.97\n",
      "Epoch 250, Overall loss = 0.187 and accuracy of 0.948\n",
      "*** Latest validation accuracy = 0.7852 ***\n",
      "*** reducing Learning rate to 1.0000000000000004e-05\n",
      "***Hyper iteration 25, Learning rate = 1.0000000000000004e-05, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.339 and accuracy of 0.89\n",
      "Epoch 251, Overall loss = 0.214 and accuracy of 0.942\n",
      "Iteration 1000: with minibatch training loss = 0.831 and accuracy of 0.95\n",
      "Epoch 252, Overall loss = 0.194 and accuracy of 0.946\n",
      "Iteration 2000: with minibatch training loss = 0.175 and accuracy of 0.97\n",
      "Epoch 253, Overall loss = 0.182 and accuracy of 0.95\n",
      "Iteration 3000: with minibatch training loss = 0.129 and accuracy of 0.94\n",
      "Epoch 254, Overall loss = 0.176 and accuracy of 0.951\n",
      "Epoch 255, Overall loss = 0.187 and accuracy of 0.949\n",
      "Iteration 4000: with minibatch training loss = 0.402 and accuracy of 0.88\n",
      "Epoch 256, Overall loss = 0.192 and accuracy of 0.948\n",
      "Iteration 5000: with minibatch training loss = 0.0774 and accuracy of 0.97\n",
      "Epoch 257, Overall loss = 0.199 and accuracy of 0.946\n",
      "Iteration 6000: with minibatch training loss = 0.221 and accuracy of 0.92\n",
      "Epoch 258, Overall loss = 0.18 and accuracy of 0.95\n",
      "Epoch 259, Overall loss = 0.161 and accuracy of 0.955\n",
      "Iteration 7000: with minibatch training loss = 0.219 and accuracy of 0.95\n",
      "Epoch 260, Overall loss = 0.191 and accuracy of 0.949\n",
      "*** Latest validation accuracy = 0.7877 ***\n",
      "*** reducing Learning rate to 1.0000000000000004e-06\n",
      "***Hyper iteration 26, Learning rate = 1.0000000000000004e-06, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.122 and accuracy of 0.97\n",
      "Epoch 261, Overall loss = 0.236 and accuracy of 0.939\n",
      "Iteration 1000: with minibatch training loss = 0.204 and accuracy of 0.91\n",
      "Epoch 262, Overall loss = 0.221 and accuracy of 0.942\n",
      "Iteration 2000: with minibatch training loss = 0.157 and accuracy of 0.94\n",
      "Epoch 263, Overall loss = 0.194 and accuracy of 0.947\n",
      "Iteration 3000: with minibatch training loss = 0.115 and accuracy of 0.95\n",
      "Epoch 264, Overall loss = 0.197 and accuracy of 0.948\n",
      "Epoch 265, Overall loss = 0.169 and accuracy of 0.954\n",
      "Iteration 4000: with minibatch training loss = 0.139 and accuracy of 0.97\n",
      "Epoch 266, Overall loss = 0.172 and accuracy of 0.954\n",
      "Iteration 5000: with minibatch training loss = 0.378 and accuracy of 0.92\n",
      "Epoch 267, Overall loss = 0.177 and accuracy of 0.954\n",
      "Iteration 6000: with minibatch training loss = 0.276 and accuracy of 0.92\n",
      "Epoch 268, Overall loss = 0.176 and accuracy of 0.953\n",
      "Epoch 269, Overall loss = 0.161 and accuracy of 0.957\n",
      "Iteration 7000: with minibatch training loss = 0.095 and accuracy of 0.95\n",
      "Epoch 270, Overall loss = 0.168 and accuracy of 0.955\n",
      "*** Latest validation accuracy = 0.7897 ***\n",
      "***Hyper iteration 27, Learning rate = 1.0000000000000004e-06, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.0988 and accuracy of 0.97\n",
      "Epoch 271, Overall loss = 0.194 and accuracy of 0.951\n",
      "Iteration 1000: with minibatch training loss = 0.088 and accuracy of 0.97\n",
      "Epoch 272, Overall loss = 0.178 and accuracy of 0.952\n",
      "Iteration 2000: with minibatch training loss = 0.371 and accuracy of 0.88\n",
      "Epoch 273, Overall loss = 0.17 and accuracy of 0.954\n",
      "Iteration 3000: with minibatch training loss = 0.0867 and accuracy of 0.97\n",
      "Epoch 274, Overall loss = 0.164 and accuracy of 0.957\n",
      "Epoch 275, Overall loss = 0.156 and accuracy of 0.958\n",
      "Iteration 4000: with minibatch training loss = 0.0266 and accuracy of 1\n",
      "Epoch 276, Overall loss = 0.149 and accuracy of 0.96\n",
      "Iteration 5000: with minibatch training loss = 0.0606 and accuracy of 0.98\n",
      "Epoch 277, Overall loss = 0.152 and accuracy of 0.958\n",
      "Iteration 6000: with minibatch training loss = 0.128 and accuracy of 0.95\n",
      "Epoch 278, Overall loss = 0.145 and accuracy of 0.961\n",
      "Epoch 279, Overall loss = 0.165 and accuracy of 0.957\n",
      "Iteration 7000: with minibatch training loss = 0.0982 and accuracy of 0.97\n",
      "Epoch 280, Overall loss = 0.143 and accuracy of 0.962\n",
      "*** Latest validation accuracy = 0.7915 ***\n",
      "***Hyper iteration 28, Learning rate = 1.0000000000000004e-06, Regularization = 0.1\n",
      "Iteration 0: with minibatch training loss = 0.0955 and accuracy of 0.95\n",
      "Epoch 281, Overall loss = 0.168 and accuracy of 0.957\n",
      "Iteration 1000: with minibatch training loss = 0.114 and accuracy of 0.97\n",
      "Epoch 282, Overall loss = 0.157 and accuracy of 0.959\n",
      "Iteration 2000: with minibatch training loss = 0.0629 and accuracy of 0.98\n",
      "Epoch 283, Overall loss = 0.175 and accuracy of 0.959\n",
      "Iteration 3000: with minibatch training loss = 0.168 and accuracy of 0.94\n",
      "Epoch 284, Overall loss = 0.194 and accuracy of 0.954\n",
      "Epoch 285, Overall loss = 0.156 and accuracy of 0.96\n",
      "Iteration 4000: with minibatch training loss = 0.0908 and accuracy of 0.97\n",
      "Epoch 286, Overall loss = 0.171 and accuracy of 0.959\n",
      "Iteration 5000: with minibatch training loss = 0.0273 and accuracy of 1\n",
      "Epoch 287, Overall loss = 0.157 and accuracy of 0.959\n",
      "Iteration 6000: with minibatch training loss = 0.149 and accuracy of 0.94\n",
      "Epoch 288, Overall loss = 0.139 and accuracy of 0.963\n",
      "Epoch 289, Overall loss = 0.146 and accuracy of 0.963\n",
      "Iteration 7000: with minibatch training loss = 0.108 and accuracy of 0.97\n",
      "Epoch 290, Overall loss = 0.148 and accuracy of 0.963\n",
      "*** Latest validation accuracy = 0.7873 ***\n",
      "*** reducing Learning rate to 1.0000000000000005e-07\n",
      " *** end of training ***\n",
      "Best average validation accuracy =0.791 using rs = 0.100 and lr = 1.00e-01\n",
      "Model saved in file: ./my-cifar10-model\n",
      " \n",
      "*** Using 60 hidden convolution layers ***\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl81PWd+PHXO8mEBIIJh2A5FGxZuQVJFVdaSXG9toIi\ni1LRqlW3VmvVrUpbRcq2W6q/KmXr2nrX1oqpB+JVu0uh1u16QIEoKIUiCgnlJgIJ5Hr//vh+J3wz\n853JZDKTud7PxyOPzHzPzycD3/d8blFVjDHGGK+8VCfAGGNM+rHgYIwxJowFB2OMMWEsOBhjjAlj\nwcEYY0wYCw7GGGPCWHAwBhARFZHPpTodxqQLCw4m7YjIFhGpF5GDnp+fpTpdxuSSglQnwJgILlDV\n/0l1ItKRiBSoalOq02Gym5UcTEYRkStF5H9F5GciUisiH4rIFM/+ASKyVET2isgmEbnWsy9fRL4r\nIn8TkQMiskpEBnsuf5aIbBSR/SLygIhIhDScKiL/5x633U1LoWf/KBH5bzcNO0Tku9HuLyJD3Gqt\nAs81VojINSF5vl9E9gDzROSzIvIHEdkjIrtF5CkRKfOcP1hEnheRXe4xPxORQjdNYzzH9ROROhE5\ntnOfjMk2FhxMJjoN+BvQF7gbeF5Eerv7FgPbgAHADOA/RORL7r5bgVnA+cAxwNVAnee6XwY+D4wF\nZgLnRLh/M3CLe//TgSnANwBEpCfwP8Dv3DR8DlgW4/3by/NmoD/wQ0CAH7n3GAEMBua5acgHXgY+\nBoYAA4HFqtrg/n1me647C1imqrtiTIfJFapqP/aTVj/AFuAgsN/zc62770qgBhDP8e8Al+M8IJuB\nnp59PwKecF9vAKZFuKcCkzzvK4E5Mab3ZuAF9/UsYHWE43zvj/MAV6DAs20FcI0nz5+0k4YLg/fF\nCVi7vNfzHHca8Enw7wesBGam+jO3n/T7sTYHk64u1MhtDtWq6p0x8mOcb9ADgL2qeiBkX7n7ejBO\niSOSv3te1wElfgeJyD8A97nX7Y7Tdrcqhnu0d/9otoakoT/wU+ALQE+cWoB9nvt8rD7tEqr6tojU\nAZNFZDtOyWZpnGkyWcyqlUwmGhjSHnA8TmmiBujtVu1491W7r7cCn03A/R8EPgSGqeoxwHdxqnmC\n9zgxwnmR7n/I/d3ds+24kGNCp0/+D3fbGDcNs0PScLy3DSPEL93jLweeVdXDEY4zOcyCg8lE/YCb\nRCQgIv+CU+f+qqpuBf4M/EhEikRkLPA14NfueY8A/y4iw8QxVkT6xHH/nsCnwEERGQ5c79n3MvAZ\nEblZRLqJSE8ROS3a/dWp768GZruN1lfTfhDriVP1VisiA4HbPPveAbYDC0Skh/u3OMOz/9fARTgB\n4sk48m9ygAUHk65eChnn8IJn39vAMGA3TuPsDFXd4+6bhVOHXwO8ANztqZ66D6ct4fc4D/dHgeI4\n0vZt4CvAAeBh4JngDrdK65+AC3CqqTYCFTHc/1qcB/weYBROkIvm+8ApQC3wCvC8Jw3N7v0/h9O+\nsA24xLN/K/AXnJLHnzqQb5NDpG3VrTHpTUSuxGmonZTqtGQyEXkMqFHVO1OdFpOerEHamBwjIkOA\n6cD41KbEpDOrVjImh4jIvwPvA/eq6kepTo9JX1atZIwxJkzSSg4i8piI7BSR9yPsFxFZ5E5xUCUi\npyQrLcYYYzommW0OTwA/I3JXufNwepwMwxm1+aD7O6q+ffvqkCFD4krQoUOH6NGjR1znpivLU/rL\ntvyA5SkThOZn1apVu1U19jm0kjn8GqdL4fsR9v0CmOV5vwH4THvXnDBhgsZr+fLlcZ+brixP6S/b\n8qNqecoEofkBVmoHnt9JbXNwe0W8rKqjffa9DCxQ1Tfd98uAO1R1pc+x1wHXAfTv33/C4sWL40rP\nwYMHKSnxnREhY1me0l+25QcsT5kgND8VFRWrVLU8yiltZERXVlV9CHgIoLy8XCdPnhzXdVasWEG8\n56Yry1P6y7b8gOUpE3Q2P6nsylqNM0FY0CCOzoFjjDEmhVJZclgK3Cgii3EaomtVdXsK05MTGhsb\n2bZtG4cPp+9ca6WlpXzwwQepTkbCZFt+oOvzVFRUxKBBgwgEAl12z1yXtOAgIk8Dk4G+IrINZ1GW\nAICq/hx4FWfRk0040yNflay0mKO2bdtGz549GTJkCBEWOku5AwcO0LNnz/YPzBDZlh/o2jypKnv2\n7GHbtm0MHTq0S+5pkhgcVHVWO/sVuCFZ9zf+Dh8+nNaBwZhQIkKfPn3YtSv3FqtbsrqaeUvXsb++\nsXVbr+4B7r5gFBeOH5jUe2dEg7RJLAsMJtNk+r9Zv4c8QJ5AizoLccTab3RfXSO3PbsWIKkBwoKD\nMcZEsWR1Nfe+voGa/fUMKCvmtnNO4sLxA30f+N0DTh+fusaW1veNzS24b8O0uBGhowMKGpuVe1/f\nkNTgYBPvmS61Z88exo0bx7hx4zjuuOMYOHBg6/uGhoaYrnHVVVexYcOGqMc88MADPPXUU4lIcsap\nqKjg9ddfb7Nt4cKFXH/99RHOoLU/fE1NDTNmzPA9ZvLkyaxcGTYMKew+dXV1re/PP/989u/fH2vS\nu8yS1dWcseAPDJ3zCmcs+ANLVlfz55pGxn3/9wyZ80rrz7DvvsLNz6yhen89ClTvr+c7z7/HnUve\n47bfrg0rCdQ1trQGhuD7SIGhs2r21yfnwi4rOZioIn1rilefPn1Ys2YNAPPmzaOkpIRvf/vbbY5R\nVVpaWsjL8//u8vjjj7d7nxtuyKDmrKpKWDYfardB6SCYMhfGzoz7crNmzWLx4sWcc845rdsWL17M\nPffc0+65AwYM4Nlnn4373gsXLmT27Nl07+6sePrqq6/Gfa1ECv47rvZ5oFbvr+fmZ9b4nuf3YK9v\nbObXb32S6CR22ICyeNapip2VHExES1ZX853n3wv71rRkdeKHo2zatImRI0dy2WWXceqpp7J9+3au\nu+46ysvLGTVqFPPnz289dtKkSaxZs4ampibKysqYM2cOJ598Mqeffjo7d+4E4M4772ThwoWtx8+Z\nM4dTTz2Vk046iT//2Vlk7dChQ1x88cWMHDmSGTNmUF5e3hq4ukxVJbx0E9RuBdT5/dJNzvY4zZgx\ng1deeaW1JLZlyxZqamoYP348U6ZM4ZRTTmHMmDG8+OKLYedu2bKF0aOdCQ3q6+u59NJLGTFiBBdd\ndBH19UcfrLfcckvrZ3P33XcDsGjRImpqaqioqKCiwln8bsiQIezevRuA++67j9GjRzN69OjWz2bL\nli2MGDGCa6+9llGjRnH22We3uU8slqyuDvvGH/wZeddrbb79Z4tAvnDbOScl9R5Wcshh339pHetr\nPo24f/Un+2lobvvVqb6xmdufreLpd/y/OY0ccAx3XzAqrvR8+OGHPPnkk5x00kn07NmTBQsW0Lt3\nb5qamqioqGDGjBmMHDmyzTm1tbWceeaZLFiwgFtvvZXHHnuMOXPmhF1bVXnnnXdYunQp8+fP53e/\n+x3/+Z//yXHHHcdzzz3H2rVrOeWUJEwM/NociqtXQ36E/2rb3oXmI223NdbDizfCql/6n3PcGDhv\nQcRb9u7dm1NPPZXXXnuNadOmsXjxYmbOnElxcTEvvPACxxxzDLt372bixIlMnTo1YmPvgw8+SPfu\n3fnggw+oqqpq8/e56667OOGEE2hubmbKlClUVVVx0003cd9997F8+XL69u3b5lqrVq3i8ccf5+23\n30ZVOe200zjzzDPp1asXGzdu5Omnn+bhhx9m5syZPPfcc8yePRuAfXUN7Kg9TENzC3tqD7NhdXWb\nkuudS96L+i2+Lll1OilkvZVMyoUGhva2d9ZnP/tZysvLOXDgAABPP/00jz76KE1NTdTU1LB+/fqw\n4FBcXMx5550HwIQJE/jTn/yXRJ4+fXrrMVu2bAHgzTff5I477gDg5JNPZtSo+IJap4QGhva2xyhY\ntRQMDo8++iiqyne/+13eeOMN8vLyqK6uZseOHRx33HG+13jjjTe46aabABg7dixjx45t3ffCCy/w\n5JNP0tTUxPbt21m/fn2b/aHefPNNLrrootZZQqdPn86f/vQnpk6dytChQxk3bhzgfD4fbPwb62pq\naW5p20zb1KLc/Mwabn92LQV5kpUPfj9dFQxCWXDIYe19wz9jwR98i+IDy4p55l9PT3h6vNMLb9y4\nkZ/+9Ke88847lJWVMXv2bN9R3YWFha2v8/PzaWpq8r12t27d2j0mKc5bQH20AWP3j3arlEKUDoar\nXon7ttOmTeOWW27hL3/5C3V1dUyYMIEnnniCXbt2sWrVKgKBAEOGDIlrpPxHH33EokWLWLVqFb16\n9eLKK6+M6zp1DU38dcenaF4BVdv2kyfCzgMN1NXVhwUGr4ZmpaE5Mxcp61GYzw8vGgPQ2gaSL0Kz\nKmXFAURgf11jQtr3OsuCg4notnNO4jvPv0d9Y3PrtuJAftLrOgE+/fRTevbsyTHHHMP27dt5/fXX\nOffccxN6jzPOOIPKykq+8IUv8N5777F+/fqEXj8mU+Y6bQyNniAcKHa2d0JJSQkVFRVcffXVzJrl\njEetra2lX79+BAIBli9fzscffxz1Gl/84hf5zW9+w5e+9CXef/99qqqqAOez6dGjB6WlpezYsYPX\nXnutdYK3nj17cuDAgbBqpS984QtceeWVzJkzh72HjrD4t8/xg4U/x1sIbcngVSnzBEqLA60P9orh\nx/Ly2u2tvZn8vv2n8sEfCwsOJqLgP95E9laK1SmnnMLIkSMZPnw4J5xwAmeccUbC7/HNb36TK664\ngpEjR7b+lJaWJvw+UQV7JSWwt1LQrFmzuOiiiwhOcX/ZZZdxwQUXMGbMGMrLyxk+fHjU86+//nqu\nuuoqRowYwYgRI5gwYQLgVMGNHTuW4cOHM3jw4NbPZl9dA9MuuYKKs86mX//jeKTyJRqblf11DQwd\nPprzp1/K2PHONabPuoIRo8dSvTW1vX7yBUILIQL842d7s67mQOvDPdogtWBpIPT/xQ8uHJPw9Hal\njFtDury8XNvrax1Jtk3JCx3P0wcffMCIESOSl6AE6Kp5e5qammhqaqKoqIiNGzdy9tlns3HjRgoK\nEvudKZvnVtpX10DN/ujVQB0Z/RvNjk82c+3SxMzNOTBkMFuwimdghC9AwZ573lK0AJdNPD5tg0Do\ns0FEsm89B2OS4eDBg0yZMoWmpiZUlV/84hcJDwzZIjQI5AFauz+mh35Xfv2cPfF4yk/ozW2/XUtj\nSMAK5Av3zjg57MF/4fiBXDh+YNQvWqksRaeK/U8wOausrIxVq1alOhlpyduF1E+69RPyq9P3Tm2R\niB4/wSCSKyw45CBVzfiJzEznhZYGCvKE0uIA++oa06JxOE+Egb2K6dW9EFUlcKCYhZeMi+nbe649\nyJPBgkOOKSoqYs+ePfTp08cCRA7bV9fAtr31qKfSp6lF2XMotvmtkq1Pj0IG9nKm4Aiu51BUVMSF\nI+yh31UsOOSYQYMGsW3btrSeG//w4cMUFRWlOhkJ09X5qWtoorausbUXTr5AcWE+9Q3NYT1zUkmA\nHt3yKSzI49P6JppblPw84ZjiAj49UMCnfz96bHAlONN1LDjkmEAgkParaa1YsYLx48enOhkJ05X5\naW86iXQR7BV0lpUC0pYFB2OygNPVsor6FE0pIQL3zxzHheMHRhxZny/CT2aG9xYy6cmCgzEZInT6\n9Irhx7L8w10pn200tItopJH1P5oePlDMpC8LDsakOb8Vx6r313dJ9VFwGctgNRC030U0F8cEZCML\nDsakqVRVFbU38re9AWPBYywYZDYLDsakma4OCt0DeXQL5KfNbKAmPVhwMCZF/KqLAnn+S1MmSrrP\nB2TShwUHY7pYtJJBogJD6LoBVvdvOsqCgzFdwK+UkAx+s4paMDDxsOBgTBJ4u512L4D65jVEmdW6\n08qKA6y5++zk3cDkHAsOxiSQX5XRoSSvShrIF+ZNTcH61yarWXAwJgGS2cPIu9JYaPVUqhafN9nP\ngoMxnZDsbqezQ3oW2fgB01UsOBjTAd4lJROleyCPxhal0TNlqnU5NalmwcGYGCS6hOCdqC54fety\natKJBQdj2pHoabD91jK26iKTbiw4mJzn960dSMq4BG/jsjHpzIKDyWl3LnmPp976pHWxzOr99dz6\nzBo6W3nUozCfi04Z2Dqltt/gNGPSmQUHk7MiVRd1JjD4lQzam8HUmHRkwcHknGRMZRHawGxySFUl\nLJsPtdugdBBMmQtjZ6Y6VZ2Wl8yLi8i5IrJBRDaJyByf/ceLyHIRWS0iVSJyfjLTY8ydS97jlmfW\nJDQwFAfyLTDkqqpKeOkmqN0KqPP7pZuc7RkuaSUHEckHHgD+CdgGvCsiS1V1veewO4FKVX1QREYC\nrwJDkpUmk9sS0euoV/cA/zz2Myz/cFd2djvNtG/BXZ3e0PvV74XGkDEvjfXOMX7p8J5f3MvZVr8v\netpT9Jkks1rpVGCTqm4GEJHFwDTAGxwUOMZ9XQrUJDE9Jsd4eyEVBfI6NUYhJ3oZBb8FBx92wW/B\nkF4BovVhuRVnuKDbnaB2Kzx/Lbx4A+QFoPGQsz3QAwq6tX0IQ/gDl34+99jW9pzQv08ktVvh/tHR\nz6/f2/Z479+6TR5Drvv8dfDJW/Dl+zrwR+u4ZAaHgYA3Z9uA00KOmQf8XkS+CfQAzkpiekwOCe2F\n1JnAEDqFRdZaNr9j34Ihed9qI103NIDhM9Vtc4Pz05qHQ0cDRe1WWPINp5EoeIwbVM4EWIETTFoa\nw/a3CUSxCD7YW89vR2O9c9ySG6ClIcqBCisfg+MnJjVoi2py5hEWkRnAuap6jfv+cuA0Vb3Rc8yt\nbhp+IiKnA48Co1W1JeRa1wHXAfTv33/C4sWL40rTwYMHKSkpievcdGV5CvfLdYdZvrU5IWmpGJzP\nV0cVdeoaqfqM+u34Iydu/hXdjuzmSLe+bD7xcoDWbY35JSAQaDrIkW596XZkF+JzneAT4ki3Y9l8\n4uXs7H8mBw8e5MRDqzhpwwPktxxpPbY5rxsbTrqhzX2OdOvL7j7l9N2zkm5HdqHkIbS0uZ43rY35\nJRS01JOnR6ezbSGfpoLuBJoO+KYxFx3udixvnf5IxP2h/+4qKipWqWp5rNdPZnA4HZinque4778D\noKo/8hyzDieAbHXfbwYmqurOSNctLy/XlStXxpWmbOxSaHlydLYHUnEgn4snDExKW0KH81NVCa/d\ncbTaobg3nPfj2L8lVlXCSzcf/bbcKo/OddSF4Lfnw92OpSivuW3ViFd+Ydtv71EvmQ+amGCeWwTm\n7Y+4N/TfnYh0KDgks1rpXWCYiAwFqoFLga+EHPMJMAV4QkRGAEXAriSmyWShzjY0p9UAtapKp9qj\nxRPk6vfCC193A0aUxsvQoBImEfNCOV8mi46089801sAAFhjiVTooqZdPWnBQ1SYRuRF4HcgHHlPV\ndSIyH1ipqkuBfwMeFpFbcP7VXanJKsqYrNSZwJC0toSQ+vJ+A/4FmBzbucvmtw0MQer5lh6sw37+\nWqdUMeoiWPdClKBgsk6g+Ggjd5IkdRCcqr6K0z3Vu22u5/V64IxkpsFkl0T1QEpqYAjp0XLSgQfg\n5VrY+PvoDbdVldF7wPip3wsrH01M2k16C1a/lQ7uku6sNkLaZAS/KbPjDQxlxYHEBgZvSUHywqpJ\n8luOOL1LvF0uQ7uIBoOKyWAd6M3UkTYZgHm1caWoMyw4mLSXyCmziwP5PDz+I/8+6MF+5dG+obVX\nrx+x/jzkoRHsIgrttBOYmOUF2nZRjUV+IRSWQP0+GvJLKMxraduQH+jh/A5r3A8RrN5b+5u23YED\nxXDyV/xLjfePjq2kWDo49vwkkAUHk9aWrK7mqTgCQ3DQGtBmOu6FIzfy+ffubjuQKbTfe/AB7zcw\nKbSxuDNi7f9uQrjf0It7O299B7eFDJAL5dMD7M/RepWFja+IcJ3jJ8Y+7mPKXP9reuUXJr1tIRIL\nDiZtLVldzS2Vazoy7AjwX3e51f0+/xmjPewb650Rqbnw7T5YYmp96O7tWDfToWfCtpXh37LzAtCt\nZ3hPq9BSWOtI5r1EfLDH0q3XW1WXqAF6wfPau97YmbHfI/Saxb2g6cjRv19HuzAnmAUHk5aiVSVN\nzXuT2wsqGSC7qdG+3NM0k6Utk4AoDc2RpiOIiWZ3YAgUwwWLIj+E5pXRbl16cW/46lK3qiQkOLQ0\nQmEPuOOjttvbe5B29uHekQd1Kq6XrGsmiAUHkxaCvZCq9we/1fvX8U7Ne5MFgUfoLk4V0CDZzb2B\nXzCPJ+klh5CPBkFVyEPk5VvbNgjnEr9v7dCxh27poOhBNVDsfMMF55p+Im2PJo0fnLnAgoNJudB5\nkKK5vaCyNTAEdZNmunHQeRPaTvDyrZnd1TO0V0ugGAqKI5RkBMqvbr/LLHTsoetTN67O3cIb7SMF\nkiQP2DKJZ8HBpEx7U154q4/2aQki0CsYBKIJTmCW6Y29wQdve7ODAq2BIRkzdfrUt38w4F8Yecnd\n4cf6NbJ2wYAtk3gWHExK+JUWQoNBTzlMoTiTr/WRGIJCNgk+UKNVrXTlHP8h6di5YgUjIx3X1Wkz\nSWHBwXQ5v8bm0LaErAwGwYbfqCUaie2Bms718emcNhMzCw6mS0XqheTXlpBWQgdDFfeG5iPQ0M7g\nqMIeaEMd4n3gR+o1VToYbnk/sek2Jk4WHExy+HRDvHPziIjdUwfK7i5OoI/CHvDlhc7rWKpFfAdG\nuf3zPQ21fwwdXGX18iYDWHAwiecz+Vz9czfwaeM1wKSww6fmvXm090uqTH+4bQCIpVok3vp1q5c3\nGcCCg0k8n+Umi6WB+wMPspD/Chu4dntBJXmdiQydXSymdHDnRs7Gc67Vy5s0Z8HBdI7PRHSRSgH5\n4vRNGiS7+Wngv/gp/9X5+wcnNgud8MyP5ENefvi4AavOMSZMXqoTYDJYcCK6kAFZsRQCRNr+xKV0\nsNP758v3Ob9LBzt3Lx0M5V87OkcQOK8v+jlMe6DtcdGmjTAmh1nJwcQv0qplSScw/aHwNoLQh3yk\nAWEWDIxpl5UcTMdUVTqTq80ri3MSuw6Y/rBT7dOGOxLYHvDGJJWVHEzsIs1pnwzeRmLr1WNMl7Pg\nYGLn0wspGZrzupEfbCS2Xj3GpIRVK5nYxTDtsqrz0xkbTrrBAoIxKWbBwcQuhmmXq7Uv32r8Btta\n+tKiQrNG6Iok+RHuMZid/c/sRCKNMYlgwcHEbtjZROuo2qAFrYPbJjUs4sQjT/HcCXeFNyoHimHC\nlf7bbcyBMWnBgoNpX1Ul/HCAu2hO5DqjA1rUOuo5aObV/xY+BiHS2AQbc2BM2rAGaRNdcKBbDOMZ\neknbGUoHlrklg0iNytbYbEzaspKDia4DA91qtE/r6+JAPredc1KyUmWMSTILDia6GBeGr9NC7mly\nSgH5Ivxo+hguHD8wmSkzxiSRBQcTXZQeSsFuq9ta+jKn8ZrW9oafzDzZAoMxGc7aHExkVZURVzpT\nhSebz+LupqvbbC8rDlhgMCYLWHAw/l6+FVY+hrd3UnBw2z5KmNd4RVjPJAHmTR3VdWk0xiSNVSuZ\ncC/f6tttVcTZEikwXDbxeCs1GJMlLDiYtqoq3RKDvzxxVm7z6tU9wP2XjOMHF45JduqMMV3EqpVM\nW6/dQbSBbgADZA/glCTunznOSgvGZCErOZijqirDVnXzU6N9ECwwGJPNrORgfNeBjiQ4nkHBAoMx\nWcyCQ66LYXqMYC+lau3bOrFe69QYxpis1G5wEJFvAr9W1X1dkB7TFaoqj66uJnmgzVEP36slTGh4\nqM02mxrDmOwWS5tDf+BdEakUkXNFJPKczSHc4zeIyCYRmRPhmJkisl5E1onIb2K9tolTcKnP2q2A\nthsY6rSQ7zdd0WabDXQzJvu1GxxU9U5gGPAocCWwUUT+Q0Q+G+08EckHHgDOA0YCs0RkZMgxw4Dv\nAGeo6ijg5ngyYTqgA0t9Nqu0mRYDbKCbMbkipt5KqqrA392fJqAX8KyI3BPltFOBTaq6WVUbgMXA\ntJBjrgUeCFZZqerODqbfdFSME+kBNBO+Wps1RBuTG0TbWfBXRL4FXAHsBh4Blqhqo4jkARtV1bcE\nISIzgHNV9Rr3/eXAaap6o+eYJcBfgTOAfGCeqv7O51rXAdcB9O/ff8LixYs7nFGAgwcPUlJSEte5\n6aqjeZr4f9dQdGRXzMdva+nLpIZFre/7FAk/mdy9Q2nsqGz7nLItP2B5ygSh+amoqFilquWxnh9L\nb6XewHRV/di7UVVbROTLMac08v2HAZOBQcAbIjJGVfeH3Osh4CGA8vJynTx5clw3W7FiBfGem646\nnKeDU92pMWITHPAWdNe0k5mc5JJDtn1O2ZYfsDxlgs7mJ5ZqpdeA1g7wInKMiJwGoKofRDmvGhjs\neT/I3ea1DViqqo2q+hFOKWJYLAk3caiqhLUda/P3LuBjDdHG5I5YgsODwEHP+4Putva8CwwTkaEi\nUghcCiwNOWYJTqkBEekL/AOwOYZrm3h0oDEa2i7gYw3RxuSWWKqVRD0NE251UrvnqWqTiNwIvI7T\nnvCYqq4TkfnASlVd6u47W0TWA83Abaq6J/JVTafUbm33kGYVBKfEEBzwBtYQbUyuiSU4bBaRmzha\nWvgGMX67V9VXgVdDts31vFbgVvfHJIJ3gFvpIJgyF8bOdKbhjoEAJx55Kmy7jYg2JrfEEhy+DiwC\n7sT5ArkMt+eQSTPBAW7BqqParfD8dfD8tTFfwtvG4GUjoo3JLbFUD+3EaS8w6c63TSF6V2UvbxuD\nlzVEG5N7YplbqQj4GjAKKApuV9WrI55kUqMDA9yCmjSPPDSsjSHIGqKNyU2xVCv9CvgQOAeYD1wG\nROvCalKldFBMjc5BLQq3Nn49LCB4WUO0Mbkplq6sn1PVu4BDqvpL4J+B05KbLBOXKXNxvuu3rwX4\nVfNZUQMDWEO0MbkqluAQnOh/v4iMBkqBfslLkonb2JnE0sbQInnc3PAN7m6KXjNYHMi3hmhjclQs\n1UoPiUitfvFhAAAVr0lEQVQvnN5KS4ES4K6kpsrEr3Rw9KqlQDHfa7yWpS0To15mYFkxt51zklUp\nGZOjogYHd3K9T91ZU98ATuySVJmOax3fECUwlA6msvQqnv7r8IiHzJ54PD+4cEwSEmiMySRRq5VU\ntQW4vYvSYuLVZgEfH4HuMP1hlkx+nTuiBIay4oAFBmMMEFubw/+IyLdFZLCI9A7+JD1lJnbtzZnU\nWAcv3cSaVx6K2iJhXVaNMUGxtDlc4v6+wbNNsSqm9BHL+IbGeq5p+TVPcKrvbhvoZozximWE9NCu\nSIjphBjHN4SuzRBkA92MMaFiGSF9hd92VX0y8ckxcRl2Nqx8jPa6sUaaN+myicdbqcEY00Ys1Uqf\n97wuAqYAfwEsOKSBfjv+CJt+Q3uBIdK8SYA1QhtjwsRSrfRN73sRKQPiW8TZJNyJm38VtTFaFaq1\nr++8SWAjoI0x/mIpOYQ6BFg7RJrodmRX1P2KMKlhUcT9NgLaGOMnljaHlzhaZ5EHjAQqk5koE6Oq\n9j+GSO0MYD2UjDGRxVJy+H+e103Ax6ra8bmhTWJ4V3qTvKjT7EVrZygO5FsPJWNMRLEEh0+A7ap6\nGEBEikVkiKpuSWrKTLjQld602few9toZenUPcPcFo6zUYIyJKJbg8FvgHz3vm91tn/c/3CSE31rQ\n7Y2EdlVr34jtDAsvGWdBwRjTrliCQ4GqNgTfqGqDiBQmMU3Gby1o7/soWpSIVUkDy4otMBhjYhLL\n3Eq7RGRq8I2ITAN2Jy9JxreEEENgCPKrShKsZ5IxJnaxlBy+DjwlIj9z328DfEdNmwSJYy3ooBrt\n67vdRkEbYzoilkFwfwMmikiJ+/5g0lOV6zq4FnRQpCola2cwxnRUu9VKIvIfIlKmqgdV9aCI9BKR\nH3RF4nLWlLkg+XGdGlqlZO0Mxph4xNLmcJ6q7g++cVeFOz95ScpxwV5K3m6qMQaK0Cola2cwxsQr\nljaHfBHppqpHwBnnAHRLbrJy1Mu3+s+uGmE8g1eDFoRVKSlYqcEYE5dYgsNTwDIReRzny+iVwC+T\nmaicVFUZ07TbkRzQIt8qJWOMiUcsDdI/FpG1wFk4T67XgROSnbCs1jrAbStOvI0vIHj1kkNt3luV\nkjGmM2KdlXUHzhPsX4CPgOeSlqJsFTEgdD4wQPgEe1alZIzpjIjBQUT+AZjl/uwGngFEVSu6KG3Z\nI3TEc4ICQpDfBHtWpWSM6YxoJYcPgT8BX1bVTQAickuXpCrbvHZHh0Y4d0ST5jGn8Zo27Q1WpWSM\n6axoXVmnA9uB5SLysIhMgagzRBs/VZVQvzfOk6P/ueu0kFsbvx7WEG2joY0xnRUxOKjqElW9FBgO\nLAduBvqJyIMicnZXJTDjLZsf54mRG6pVYVtL37ASA8DsicfbmtDGmE5rdxCcqh5S1d+o6gXAIGA1\ncEfSU5Yt4ponSaD8aigd7Ls3OCV3aGAoKw5YYDDGJEQsI6Rbqeo+VX1IVackK0FZpaoSJIaauEAP\nKO4NiBMQpj8EX74PpsylKb+ozaGRVncTsJXdjDEJE2tXVtNRVZWw5BugLdGPK+4Nd3zku+vOzSP4\ntP5qbi+oZIDsoUb7RFzdzdoZjDGJlNTgICLnAj8F8oFHVHVBhOMuBp4FPq+qK5OZpi6zbD60NEY/\nJlAM5/3Yd9edS97j1299AkxiaUN4MAgS4H6bddUYk2BJCw4ikg88APwTzhoQ74rIUlVdH3JcT+Bb\nwNvJSkuXq6psf8rt0sHO7Ktj21YRLVldzbyl69hf305gwQkM144ttMBgjEm4ZJYcTgU2qepmABFZ\nDEwD1occ9+/Aj4HbkpiWrhMc8BZN6WC45f02m5asruY7z1dR39hONZTHZROP5x/L9sSTSmOMiUpU\nEztat/XCIjOAc1X1Gvf95cBpqnqj55hTgO+p6sUisgL4tl+1kohcB1wH0L9//wmLFy+OK00HDx6k\npKQkrnNjNfH/rqHoyK6I+1ukgA+H38TO/mcC8OeaRp54r4GGDn4MFYPz+eqooi7JU1fLtjxlW37A\n8pQJQvNTUVGxSlXLYz0/ZQ3SIpIH3Iczy2tUqvoQ8BBAeXm5Tp48Oa57rlixgnjPjf0mUZbXLuxB\n3pcXMnLsTEbilBYe//1aGjsQGHoU5vPDi8a0ViV1SZ66WLblKdvyA5anTNDZ/CQzOFQD3o76g9xt\nQT2B0cAKcbp7HgcsFZGpGd0oHW2Jz+LerW0MS1ZXc0vlGmItuAlONZKNYzDGdIUOjXPooHeBYSIy\nVEQKgUuBpcGdqlqrqn1VdYiqDgHeAjIjMFRVwv2jYV6Z87uq8ui+KXMjn+cOiFuyuprbfrs25sDQ\nq3uA+y8ZZ4HBGNNlkhYcVLUJuBFn/YcPgEpVXSci80VkarLum3TBBufarYA6v5+/Fn481Nk3diYU\nRqi3LB0EwPdfWkdjS2yRYfbE41k992zrkWSM6VJJbXNQ1VeBV0O2+X61VtXJyUxLwiyb7z/Dav1e\nZ9Dba3dAw8Hw/YFimDKXO5e8x7669ruphrYtGGNMV7IR0h0Vba6klsaQGVjdyfPcMQ13bh7hDmyL\nTATun2mD2owxqZXMNofs5FYNxUZbxzTEEhgC+WKBwRiTFiw4dFS0Bmc/tds8U2FEJgL3zjjZAoMx\nJi1YcOiosTPdGVRjsy/Qr/3AgFUlGWPSiwWHeJz3Y5D2/3QN0o27D13c7nE2o6oxJt1YcIjH2JnQ\nox/RlvFslgJuO/I13+m1vWzlNmNMOrLeSu2pqnS6p3p7IRX3huZGOGES1Kz07dqq2hJhkc+jLDAY\nY9KVlRyiCS7Y06Z7Ks77hgNQ2B0uWASSH3ZqAS3cF/g5U/Pe9L20BQZjTDqz4BBNewv2bH3LqWKK\nsNpbgbSwIPBIWICwwGCMSXcWHCKJZcGew7XO7yhjH7pLA7cXHJ17yQKDMSYTWHDwE8uCPQCBHs7v\nKXNpyi+KeNgAcRbkKSsOWGAwxmQECw5+Is2fFKr5CFRVsqT5DG478jWa1P/PWaN9EGDe1FGJTacx\nxiSJ9VbyE23+JK+WJlg2n3uPLKK66QyaW5QFgUfoLg2th9RpIfc0zbSxDMaYjGLBwU+0BXtCaO1W\nqg87pYylLZOgEW4vqGSA7KFG+3BP00ze6FbBGqtOMsZkEAsOfqbMhRdvgOaGdg9Vhal5b7YOdlva\nMomlDUcHvglwv1UnGWMyjLU5hAoOegsNDGMvxW9EdJ7QpjdSKKtOMsZkIis5eAUHvfmNbdj4e4gw\n5jnYG8mrV/cAd18wygKDMSYjWXDwijborX4vrYv3hKjRPm3eDywr5n/nfCnx6TPGmC5i1Upe7fZS\nUkKrloK9kbxuO+ekxKbLGGO6mAUHr5hWeVP2BfrTosK2lr7MabymzcyrZcUBq0oyxmQ8q1bymjIX\nXvg6aHPEQ/YF+jP+wP2++2ygmzEmW1jJwWvsTBj3lYi7m/KLmBdl8R4FKzUYY7KCBYdQPY6FvAK4\naw9MfxhKBwMCpYP5gXydF6Ms3jOwrLjr0mmMMUlkwcGrqhLeftCZFmPROGfbLe/DvP1wy/s8cfDU\niKcK1hBtjMkeFhyCgjOxBifcq93qvK9yBrjdueS9qKfbYDdjTDaxBukgv5lYG+th2XyWNJ/BU299\nEvFUW6PBGJNtrOQQFGmMQ+02vv/SuqjrQVtgMMZkGwsOQRHGONQVH8e+ushLhVojtDEmG1lwCBp2\ndvi2QDH3NF4S8RRrhDbGZCsLDuA0Oq/9TchGgZO/ErWHkjVCG2OylQUHcKboDlsWVKlb96rPJN0O\nWw/aGJPNLDhUVbozroYrqv+7b0O0TZNhjMl2FhyWzY+4q6alj+92mybDGJPtLDhE6MKqwL0hU3EH\nWQ8lY0y2s+AQoQvrfkp851GyHkrGmFxgwcGnC2tTfhF3N1zhe7hVKRljckFuB4cIXVifbT6zzQI+\nXlalZIzJBUkNDiJyrohsEJFNIjLHZ/+tIrJeRKpEZJmInJDM9ITxm08JZVLLqoinWJWSMSYXJC04\niEg+8ABwHjASmCUiI0MOWw2Uq+pY4FngnmSlx1eExugBssd3uy0BaozJFcksOZwKbFLVzaraACwG\npnkPUNXlqlrnvn0LiGUR58SJ0Bhdo/5dWG1sgzEmV4hqtPlGO3FhkRnAuap6jfv+cuA0Vb0xwvE/\nA/6uqj/w2XcdcB1A//79JyxevDiuNB08eJCSkpLW9/12/JHhH/6UPM+a0XVayJzGa8LaHHoUwANn\n9YjrvskUmqdskG15yrb8gOUpE4Tmp6KiYpWqlsd6flqs5yAis4Fy4Ey//ar6EPAQQHl5uU6ePDmu\n+6xYsYI251bthA/zACc47KcncxsvDwsMAvzw4nFMTsMqpbA8ZYFsy1O25QcsT5mgs/lJZnCoBgZ7\n3g9yt7UhImcB3wPOVNUjSUxPW8GV3/TodNyFEW5v3VeNMbkmmW0O7wLDRGSoiBQClwJLvQeIyHjg\nF8BUVd2ZxLSE85lsr7s0cHtBZdih1n3VGJNrkhYcVLUJuBF4HfgAqFTVdSIyX0SmuofdC5QAvxWR\nNSKyNMLlEivKZHuhPZVsRLQxJhcltc1BVV8FXg3ZNtfz+qxk3j+iaJPthfRUsiolY0wuys0R0pEm\n21O4J2SyPatSMsbkotwLDlWVIP7Z3qslYT2VrErJGJOL0qIra1fpt+OP8L8PgmdcQ1CdFvL9praT\n7dmIaGNMrsqpksOJm3/lM5cSNGle2MA3W+3NGJPLcio4dDuy23d7HhpWnWQN0caYXJZTweFIt76+\n2/3mUrKGaGNMLsup4LD5xMsh0PahX6eFYT2UAvliDdHGmJyWU8FhZ/8z4YJFUFAEwN851n+SvcIC\nq1IyxuS0nOqtBMDYmfC/i6B0EBOrLvc9pLa+0Xe7McbkipwqObT6tJrNDaVIhN0DrL3BGJPjcio4\n9NvxR7hvFNTvpe+WpVyQ92bYMTaXkjHG5FK1UlUlJ214AFqcabmPoY4FgUegkTZtDtaF1Rhjcqnk\nsGw++S1t12vwm6LburAaY0yOBId3l/4Crd3quy90im6rUjLGmBwIDu8u/QUnr/pOxMZn7wA4m0vJ\nGGMcWR8cBv/lXgolfKI9gBbPFN3FgXybS8kYY1xZHxz66a6o+4ON0T+aPsZKDcYY48r64LBTjo24\nr0aduZYGlhVbYDDGGI+sDw5bT7mNI5oftr1BC7inaaaNazDGGB9ZHxw+P/VfefGE77GnpQRVZynQ\nvVrCtxuvY2nLJC6beLyVGowxJkRODIKbefW/sWT1pXzvuTUcanK29eoeYOEFoywwGGOMj5wIDuCM\nei6r3cjkyZNTnRRjjEl7WV+tZIwxpuMsOBhjjAljwcEYY0wYCw7GGGPCWHAwxhgTRlQ11WnoEBHZ\nBXwc5+l9gd0JTE46sDylv2zLD1ieMkFofk5Q1chTRoTIuODQGSKyUlXLU52ORLI8pb9syw9YnjJB\nZ/Nj1UrGGGPCWHAwxhgTJteCw0OpTkASWJ7SX7blByxPmaBT+cmpNgdjjDGxybWSgzHGmBhYcDDG\nGBMmZ4KDiJwrIhtEZJOIzEl1euIhIltE5D0RWSMiK91tvUXkv0Vko/u7V6rTGY2IPCYiO0Xkfc82\n3zyIY5H7mVWJyCmpS3lkEfI0T0Sq3c9qjYic79n3HTdPG0TknNSkOjIRGSwiy0VkvYisE5Fvudsz\n9nOKkqdM/pyKROQdEVnr5un77vahIvK2m/ZnRKTQ3d7Nfb/J3T8k6g1UNet/gHzgb8CJQCGwFhiZ\n6nTFkY8tQN+QbfcAc9zXc4Afpzqd7eThi8ApwPvt5QE4H3gNEGAi8Haq09+BPM0Dvu1z7Ej33183\nYKj77zI/1XkISeNngFPc1z2Bv7rpztjPKUqeMvlzEqDEfR0A3nb//pXApe72nwPXu6+/AfzcfX0p\n8Ey06+dKyeFUYJOqblbVBmAxMC3FaUqUacAv3de/BC5MYVrapapvAHtDNkfKwzTgSXW8BZSJyGe6\nJqWxi5CnSKYBi1X1iKp+BGzC+feZNlR1u6r+xX19APgAGEgGf05R8hRJJnxOqqoH3bcB90eBLwHP\nuttDP6fg5/csMEVEJNL1cyU4DAS2et5vI/o/jHSlwO9FZJWIXOdu66+q293Xfwf6pyZpnRIpD5n+\nud3oVrM85qnuy6g8uVUP43G+lWbF5xSSJ8jgz0lE8kVkDbAT+G+cEs5+VXXXvGyT7tY8uftrgT6R\nrp0rwSFbTFLVU4DzgBtE5IveneqUFzO6b3I25MH1IPBZYBywHfhJapPTcSJSAjwH3Kyqn3r3Zern\n5JOnjP6cVLVZVccBg3BKNsMTde1cCQ7VwGDP+0HutoyiqtXu753ACzj/GHYEi/Du752pS2HcIuUh\nYz83Vd3h/sdtAR7maJVERuRJRAI4D9GnVPV5d3NGf05+ecr0zylIVfcDy4HTcar1gktAe9Pdmid3\nfymwJ9I1cyU4vAsMc1vxC3EaY5amOE0dIiI9RKRn8DVwNvA+Tj6+6h72VeDF1KSwUyLlYSlwhdsb\nZiJQ66nWSGshde4X4XxW4OTpUrfnyFBgGPBOV6cvGrce+lHgA1W9z7MrYz+nSHnK8M/pWBEpc18X\nA/+E05ayHJjhHhb6OQU/vxnAH9wSoL9Ut7h31Q9Oj4q/4tTJfS/V6Ykj/Sfi9J5YC6wL5gGnznAZ\nsBH4H6B3qtPaTj6exim+N+LUh34tUh5wemM84H5m7wHlqU5/B/L0KzfNVe5/ys94jv+em6cNwHmp\nTr9PfibhVBlVAWvcn/Mz+XOKkqdM/pzGAqvdtL8PzHW3n4gTyDYBvwW6uduL3Peb3P0nRru+TZ9h\njDEmTK5UKxljjOkACw7GGGPCWHAwxhgTxoKDMcaYMBYcjDHGhLHgYEwIEWn2zNK5RhI4i6+IDPHO\n3mpMuipo/xBjck69OlMSGJOzrORgTIzEWU/jHnHW1HhHRD7nbh8iIn9wJ29bJiLHu9v7i8gL7nz7\na0XkH91L5YvIw+4c/L93R7cak1YsOBgTrjikWukSz75aVR0D/AxY6G77T+CXqjoWeApY5G5fBPxR\nVU/GWe9hnbt9GPCAqo4C9gMXJzk/xnSYjZA2JoSIHFTVEp/tW4AvqepmdxK3v6tqHxHZjTPtQqO7\nfbuq9hWRXcAgVT3iucYQ4L9VdZj7/g4goKo/SH7OjImdlRyM6RiN8LojjnheN2NtfyYNWXAwpmMu\n8fz+P/f1n3Fm+gW4DPiT+3oZcD20LspS2lWJNKaz7BuLMeGK3dW1gn6nqsHurL1EpArn2/8sd9s3\ngcdF5DZgF3CVu/1bwEMi8jWcEsL1OLO3GpP2rM3BmBi5bQ7lqro71WkxJtmsWskYY0wYKzkYY4wJ\nYyUHY4wxYSw4GGOMCWPBwRhjTBgLDsYYY8JYcDDGGBPm/wOda6hkU8ULJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d69534f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "print ('*** Starting ***')\n",
    "Train_accuracy_m = []\n",
    "Val_accuracy_m = []\n",
    "Continue_train = False\n",
    "num_epochs = 10\n",
    "num_mega_epochs = 1000\n",
    "i = 0\n",
    "best_accuracy = 0.0\n",
    "j = 1\n",
    "lr = 10**(-1*j)\n",
    "rs = 0.1\n",
    "optimizer = tf.train.RMSPropOptimizer(lr,epsilon=1e-3) \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vars   = tf.trainable_variables()\n",
    "lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name ]) * rs\n",
    "mean_loss_reg = mean_loss + lossL2\n",
    "\n",
    "for i in range(num_mega_epochs):                  \n",
    "    print ('***Hyper iteration %d, Learning rate = %s, Regularization = %s'% (i,lr, rs))\n",
    "       \n",
    "    _, _,Train_accuracy, Val_accuracy = run_model(sess,y_out,mean_loss_reg, \\\n",
    "                                    X_train,y_train,epochs=num_epochs, \\\n",
    "                                    batch_size=64, print_every=1000,\\\n",
    "                                    training=train_step,\\\n",
    "                                    plot_losses=False, mega_epoch=i)\n",
    "    Train_accuracy_m.extend(Train_accuracy)\n",
    "    Val_accuracy_m.extend(Val_accuracy)\n",
    "    last_run_accuracy = sum(Val_accuracy) / len(Val_accuracy)\n",
    "    print ('*** Latest validation accuracy = %s ***'% last_run_accuracy)\n",
    "    if (last_run_accuracy - best_accuracy < 0.001):\n",
    "        lr *= 0.1\n",
    "        print ('*** reducing Learning rate to %s'% lr )\n",
    "    if last_run_accuracy > best_accuracy:\n",
    "        best_accuracy = last_run_accuracy\n",
    "    if (lr < 0.000001):\n",
    "        print (' *** end of training ***')\n",
    "        break\n",
    "\n",
    "print('Best average validation accuracy =%.3f using rs = %.3f and lr = %.2e' % \\\n",
    "                  (best_accuracy, best_rs,  best_lr)) \n",
    "\n",
    "save_path = saver.save(sess, './my-cifar10-model')\n",
    "print (\"Model saved in file: %s\" % save_path)\n",
    "print (' ')\n",
    "#\n",
    "\n",
    "i = 0\n",
    "tvar = tf.trainable_variables()\n",
    "for var in tvar:\n",
    "    if (str(var).find('W:0') >= 0):\n",
    "        i +=1\n",
    "print ('*** Using %d hidden convolution layers ***' % i)\n",
    "\n",
    "plt.plot(Train_accuracy_m,'-o', label='Training')\n",
    "plt.plot(Val_accuracy_m,'-o', label='Validation')\n",
    "plt.grid(True)\n",
    "plt.title('Epoch accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper center', ncol=4)\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Wconv1:0' shape=(7, 7, 3, 32) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv2:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv3:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv4:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
      "<tf.Variable 'W1:0' shape=(6272, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'W2:0' shape=(2048, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'W3:0' shape=(2048, 10) dtype=float32_ref>\n",
      "<tf.Variable 'conv_0/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_3/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_4/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_5/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_6/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_7/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_8/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_9/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_10/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_11/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_12/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_13/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_14/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_15/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_16/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_17/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_18/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_19/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_20/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_21/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_22/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_23/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_24/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_25/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_26/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_27/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_28/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_29/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_30/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_31/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_32/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_33/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_34/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_35/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_36/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_37/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_38/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_39/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_40/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_41/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_42/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_43/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_44/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_45/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_46/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_47/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_48/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_49/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv1:0' shape=(7, 7, 3, 32) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv2:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv3:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'Wconv4:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
      "<tf.Variable 'W1:0' shape=(6272, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'W2:0' shape=(2048, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'W3:0' shape=(2048, 10) dtype=float32_ref>\n",
      "<tf.Variable 'conv_0/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_3/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_4/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_5/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_6/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_7/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_8/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_9/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_10/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_11/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_12/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_13/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_14/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_15/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_16/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_17/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_18/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_19/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_20/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_21/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_22/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_23/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_24/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_25/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_26/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_27/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_28/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_29/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_30/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_31/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_32/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_33/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_34/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_35/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_36/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_37/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_38/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_39/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_40/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_41/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_42/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_43/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_44/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_45/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_46/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_47/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_48/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'conv_49/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "*** Using 114 hidden convolution layers ***\n"
     ]
    }
   ],
   "source": [
    " \n",
    "i = 0\n",
    "for var in tvar:\n",
    "    if (str(var).find('W') >= 0):\n",
    "        print (var)\n",
    "        i +=1\n",
    "print ('*** Using %d hidden convolution layers ***' % i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18020408163265306, 0.24467346938775511], [0.30093877551020409, 0.3448979591836735], [0.38483673469387752, 0.41610204081632651], [0.44422448979591839, 0.46710204081632656], [0.48581632653061224, 0.51214285714285712]]\n"
     ]
    }
   ],
   "source": [
    "print (Train_accuracy_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-cifar10-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-cifar10-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy =0.791 using rs = 0.100 and lr = 1.00e-01\n",
      "Training dataset\n",
      "Overall loss = 0.0374 and accuracy of 0.99\n",
      "Validation dataset\n",
      "Overall loss = 2.29 and accuracy of 0.792\n",
      "Test dataset\n",
      "Overall loss = 2.43 and accuracy of 0.775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.4282932419776917, 0.7752, [0.7752], [0.747])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 70% accuracy on Validation\n",
    "\n",
    "#Restore the best network \n",
    "# clear old variables\n",
    "#tf.reset_default_graph()\n",
    "\n",
    " \n",
    "nsaver = tf.train.import_meta_graph('./my-cifar10-model.meta')\n",
    "nsaver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "print('Best accuracy =%.3f using rs = %.3f and lr = %.2e' % \\\n",
    "                  (best_accuracy, best_rs, best_lr))\n",
    " \n",
    "print('Training dataset')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation dataset')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n",
    "print('Test dataset')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did here\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tell us here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - Do this only once\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Overall loss = 3.76 and accuracy of 0.777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.7592295074462889, 0.7772, [0.7772], [0.76200000000000001])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further with TensorFlow\n",
    "\n",
    "The next assignment will make heavy use of TensorFlow. You might also find it useful for your projects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
